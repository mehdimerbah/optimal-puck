environment:
  max_episode_steps: 1000
  name: LunarLander-v2
evaluation:
  max_steps_per_episode: 1000
  metrics:
  - mean_return
  - success_rate
  - episode_length
  n_episodes: 10
models:
  dqn:
    training:
      batch_size: 64
      exploration_strategy:
        epsilon_decay: 0.995
        epsilon_min: 0.1
        epsilon_start: 1.0
      gamma: 0.99
      learning_rate: 1e-3
      replay_buffer_size: 100000
      target_update_freq: 50
    wandb_sweep:
      method: grid
      metric:
        goal: maximize
        name: evaluation/mean_return
      parameters:
        batch_size:
          values:
          - 64
          - 128
        gamma:
          values:
          - 0.95
          - 0.99
        learning_rate:
          values:
          - 1e-3
          - 5e-4
          - 1e-4
        target_update_freq:
          values:
          - 10
          - 50
          - 100
training:
  checkpoint_frequency: 100
  early_stopping_patience: 10
  eval_frequency: 50
  logging_frequency: 10
  max_steps_per_episode: 1000
  n_episodes: 500
  reward_threshold: 200
