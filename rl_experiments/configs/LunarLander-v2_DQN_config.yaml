# Environment configurations
environment:
  name: "LunarLander-v2"
  max_episode_steps: 1000

# Training configurations
training:
  n_episodes: 500  # Total number of episodes for training
  max_steps_per_episode: 1000
  eval_frequency: 50  # Evaluate the model every 50 episodes
  checkpoint_frequency: 100  # Save model checkpoint every 100 episodes
  early_stopping_patience: 10  # Number of evaluations without improvement before stopping
  logging_frequency: 10  # Log training metrics every 10 episodes
  reward_threshold: 200  # Average reward threshold to consider training successful

# algorithms configurations and sweep settings
models:
  dqn:
    wandb_sweep:
      method: "grid"
      metric:
        name: "eval/mean_return"
        goal: "maximize"
      parameters:
        learning_rate:
          values: [1e-3, 5e-4, 1e-4]
        batch_size:
          values: [64, 128]
        gamma:
          values: [0.95, 0.99]
        target_update_freq:
          values: [10, 50, 100]

    training:
      learning_rate: 1e-3
      batch_size: 64
      gamma: 0.99
      target_update_freq: 50
      replay_buffer_size: 100000
      exploration_strategy:
        epsilon_start: 1.0
        epsilon_min: 0.1
        epsilon_decay: 0.995

  # ppo:
  #   wandb_sweep:
  #     method: "bayes"
  #     metric:
  #       name: "eval/mean_return"
  #       goal: "maximize"
  #     parameters:
  #       learning_rate:
  #         distribution: "log_uniform"
  #         min: 1e-5
  #         max: 1e-3
  #       batch_size:
  #         values: [64, 128]
  #       gamma:
  #         values: [0.95, 0.99]
  #       n_epochs:
  #         values: [3, 5]
  #       clip_range:
  #         values: [0.1, 0.2, 0.3]

  #   training:
  #     learning_rate: 3e-4
  #     batch_size: 64
  #     gamma: 0.99
  #     n_epochs: 3
  #     clip_range: 0.2

# Evaluation configurations
evaluation:
  n_episodes: 10  # Number of episodes for evaluation
  max_steps_per_episode: 1000
  #save_gifs: true  # Save evaluation episodes as videos
  metrics:
    - "mean_return"
    - "success_rate"
    - "episode_length"

# Results aggregation
results:
  save_format: "json"
  grouping:
    - "environment"
    - "algorithm"
  metrics_to_aggregate:
    - "mean_return"
    - "success_rate"
    - "training_time"
  plot_types:
    - "reward_curve"
    - "training_time_comparison"

# Resource management
resources:
  gpu_ids: [0, 1]
  num_workers: 4
  max_memory: "32GB"
  timeout: 36000  # 10 hours in seconds
