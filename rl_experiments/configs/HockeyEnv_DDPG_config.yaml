environment:
  name: "HockeyEnv"
  max_episode_steps: 250
  
training:
  max_episodes: 2000        
  max_timesteps: 250        
  seed: 42                  
  log_interval: 20          
  save_interval: 500        
  render: false             
  train_iter: 32            

model:
  name: "DDPG"
    # DDPG-specific hyperparameters
  config:
    eps: 0.1                      # Noise scale
    discount: 0.95               
    buffer_size: 1000000         
    batch_size: 128              
    learning_rate_actor: 1e-4
    learning_rate_critic: 1e-3
    hidden_sizes_actor: [128, 128]
    hidden_sizes_critic: [128, 128, 64]
    update_target_every: 1000
    use_target_net: true

  wandb_sweep_config:
    name: "HockeyEnv_DDPG"
    method: "grid"
    metric:
      name: "average_reward"
      goal: "maximize"
    parameters:
      batch_size:
        values: [64, 128, 256]
      buffer_size:
        values: [1000000]
      learning_rate_actor:
        values: [1e-3, 5e-4, 1e-4]
      learning_rate_critic:
        values: [1e-3, 5e-4, 1e-4]
      discount:
        values: [0.95, 0.99]
      eps:
        values: [0.1, 0.2, 0.3]

evaluation:
  n_episodes: 10
  max_steps_per_episode: 200
  metrics:
    - "mean_return"
    - "episode_length"
