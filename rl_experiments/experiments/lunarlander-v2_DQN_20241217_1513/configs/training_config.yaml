environment:
  max_episode_steps: 1000
  name: LunarLander-v2
model:
  training:
    batch_size: 64
    exploration_strategy:
      epsilon_decay: 0.995
      epsilon_min: 0.1
      epsilon_start: 1.0
    gamma: 0.99
    learning_rate: 1e-3
    replay_buffer_size: 100000
    target_update_freq: 50
  wandb_sweep:
    method: grid
    metric:
      goal: maximize
      name: eval/mean_return
    parameters:
      batch_size:
        values:
        - 64
        - 128
      gamma:
        values:
        - 0.95
        - 0.99
      learning_rate:
        values:
        - 1e-3
        - 5e-4
        - 1e-4
      target_update_freq:
        values:
        - 10
        - 50
        - 100
training:
  checkpoint_frequency: 100
  early_stopping_patience: 10
  eval_frequency: 50
  logging_frequency: 10
  max_steps_per_episode: 1000
  n_episodes: 500
  reward_threshold: 200
