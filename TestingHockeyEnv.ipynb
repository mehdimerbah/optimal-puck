{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Imports ---\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# If your code is in a local directory, you might need to add it to sys.path:\n",
    "# (Adjust the path to where your environment code or models are located)\n",
    "# sys.path.append(\"/path/to/your/project\")\n",
    "\n",
    "# Example: from your module that implements HockeyEnv_BasicOpponent\n",
    "from hockey.hockey_env import HockeyEnv_BasicOpponent, Mode\n",
    "\n",
    "\n",
    "# Import your DDPG modules (Agent/Trainer) from your project structure\n",
    "try:\n",
    "    from models.ddpg.DDPG import DDPGAgent\n",
    "    from models.ddpg.DDPGTrainer import DDPGTrainer\n",
    "except ImportError:\n",
    "    print(\"Could not import your DDPG modules. Adjust your paths accordingly.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box(-inf, inf, (18,), float32)\n",
      "Action space: Box(-1.0, 1.0, (4,), float32)\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Initialize the Environment & Check Spaces ---\n",
    "\n",
    "# The single-agent wrapper environment is `HockeyEnv_BasicOpponent`.\n",
    "# By default:\n",
    "#   - It uses mode=Mode.NORMAL\n",
    "#   - keep_mode=True\n",
    "#   - An opponent with `weak_opponent=False` or True\n",
    "#\n",
    "# This environment has an action_space of shape (4,) \n",
    "# suitable for a single-agent continuous control algorithm like DDPG.\n",
    "\n",
    "env = HockeyEnv_BasicOpponent(\n",
    "    mode=Mode.NORMAL,   # or Mode.TRAIN_SHOOTING, Mode.TRAIN_DEFENSE\n",
    "    weak_opponent=False # whether the opponent is weaker or not\n",
    ")\n",
    "\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)\n",
    "\n",
    "# For reference:\n",
    "# - The observation space is Box(...) with shape (18,) if keep_mode=True.\n",
    "# - The action space is Box(...) with shape (4,). \n",
    "#   The four actions: \n",
    "#       1) Force in x, \n",
    "#       2) Force in y, \n",
    "#       3) Torque (racket rotation),\n",
    "#       4) Shoot command (0 or 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DDPGTrainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m experiment_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrl_experiments/experiments/HockeyEnv_DDPG_Test\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Initialize trainer\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m trainer \u001b[38;5;241m=\u001b[39m DDPGTrainer(\n\u001b[1;32m     40\u001b[0m     env_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHockeyEnv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     41\u001b[0m     env\u001b[38;5;241m=\u001b[39menv,\n\u001b[1;32m     42\u001b[0m     training_config\u001b[38;5;241m=\u001b[39mtraining_config,\n\u001b[1;32m     43\u001b[0m     model_config\u001b[38;5;241m=\u001b[39mmodel_config,\n\u001b[1;32m     44\u001b[0m     experiment_path\u001b[38;5;241m=\u001b[39mexperiment_path,\n\u001b[1;32m     45\u001b[0m     wandb_run\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# or a Weights & Biases run object if you use wandb\u001b[39;00m\n\u001b[1;32m     46\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DDPGTrainer' is not defined"
     ]
    }
   ],
   "source": [
    "# --- 4. Configure & Instantiate the Trainer ---\n",
    "\n",
    "# The DDPGTrainer (or your own trainer) expects:\n",
    "#  - environment name\n",
    "#  - training_config (like #episodes, #timesteps, etc.)\n",
    "#  - model_config (DDPG hyperparameters)\n",
    "#  - experiment_path (where logs/stats are saved)\n",
    "#\n",
    "# We demonstrate usage with your existing trainer code.\n",
    "# If your trainer requires a 'env_name' that typically does `gym.make(env_name)`,\n",
    "#   you can pass a dummy name and then directly assign `trainer.env = env`.\n",
    "\n",
    "training_config = {\n",
    "    \"max_episodes\": 2000,     # Number of episodes for training\n",
    "    \"max_timesteps\": 250,    # Max steps per episode (the environment uses ~250 for normal mode)\n",
    "    \"log_interval\": 10,\n",
    "    \"save_interval\": 100,\n",
    "    \"render\": False,         # Set True to see the environment window\n",
    "    \"train_iter\": 32,        # How many DDPG updates each episode\n",
    "    \"seed\": 42               # For reproducibility\n",
    "}\n",
    "\n",
    "model_config = {\n",
    "    \"eps\": 0.1,                  # Noise scale\n",
    "    \"discount\": 0.95,            # Discount factor\n",
    "    \"buffer_size\": int(1e5),     # Replay buffer size\n",
    "    \"batch_size\": 64,            # Minibatch size\n",
    "    \"learning_rate_actor\": 1e-4, \n",
    "    \"learning_rate_critic\": 1e-3,\n",
    "    \"hidden_sizes_actor\": [128, 128],\n",
    "    \"hidden_sizes_critic\": [128, 128, 64],\n",
    "    \"update_target_every\": 100,\n",
    "    \"use_target_net\": True\n",
    "}\n",
    "\n",
    "experiment_path = \"rl_experiments/experiments/HockeyEnv_DDPG_Test\"\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = DDPGTrainer(\n",
    "    env_name=\"HockeyEnv\",\n",
    "    training_config=training_config,\n",
    "    model_config=model_config,\n",
    "    experiment_path=experiment_path,\n",
    "    wandb_run=None  # or a Weights & Biases run object if you use wandb\n",
    ")\n",
    "\n",
    "# # Overwrite the default environment in trainer with our custom env:\n",
    "# trainer.env = env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-14 16:35:01 [INFO] Starting DDPG Training...\n",
      "2025-01-14 16:35:01 [INFO] Environment: HockeyEnv_BasicOpponent, max_episodes=2000, max_timesteps=250, train_iter=32\n",
      "2025-01-14 16:35:02 [INFO] Episode 10\tAvg Length: 171.20\tAvg Reward: -23.683\n",
      "2025-01-14 16:35:03 [INFO] Episode 20\tAvg Length: 176.20\tAvg Reward: -27.634\n",
      "2025-01-14 16:35:04 [INFO] Episode 30\tAvg Length: 184.60\tAvg Reward: -22.109\n",
      "2025-01-14 16:35:04 [INFO] Episode 40\tAvg Length: 206.10\tAvg Reward: -25.396\n",
      "2025-01-14 16:35:05 [INFO] Episode 50\tAvg Length: 167.90\tAvg Reward: -16.978\n",
      "2025-01-14 16:35:06 [INFO] Episode 60\tAvg Length: 201.50\tAvg Reward: -20.222\n",
      "2025-01-14 16:35:07 [INFO] Episode 70\tAvg Length: 169.40\tAvg Reward: -16.232\n",
      "2025-01-14 16:35:08 [INFO] Episode 80\tAvg Length: 177.10\tAvg Reward: -18.295\n",
      "2025-01-14 16:35:09 [INFO] Episode 90\tAvg Length: 195.80\tAvg Reward: -18.544\n",
      "2025-01-14 16:35:10 [INFO] Saved checkpoint at episode 100 -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/saved_models/DDPG_HockeyEnv_BasicOpponent_eps0.1_alr0.0001_clr0.001_gamma0.95_checkpoint_ep100.pth\n",
      "2025-01-14 16:35:10 [INFO] Saved training statistics to -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/stats/DDPG_HockeyEnv_BasicOpponent_eps0.1_alr0.0001_clr0.001_gamma0.95_stats.pkl\n",
      "2025-01-14 16:35:10 [INFO] Episode 100\tAvg Length: 190.00\tAvg Reward: -28.171\n",
      "2025-01-14 16:35:11 [INFO] Episode 110\tAvg Length: 147.60\tAvg Reward: -28.059\n",
      "2025-01-14 16:35:12 [INFO] Episode 120\tAvg Length: 151.80\tAvg Reward: -26.022\n",
      "2025-01-14 16:35:13 [INFO] Episode 130\tAvg Length: 179.80\tAvg Reward: -26.051\n",
      "2025-01-14 16:35:14 [INFO] Episode 140\tAvg Length: 174.70\tAvg Reward: -29.434\n",
      "2025-01-14 16:35:15 [INFO] Episode 150\tAvg Length: 210.60\tAvg Reward: -26.187\n",
      "2025-01-14 16:35:16 [INFO] Episode 160\tAvg Length: 188.90\tAvg Reward: -27.196\n",
      "2025-01-14 16:35:17 [INFO] Episode 170\tAvg Length: 164.90\tAvg Reward: -26.933\n",
      "2025-01-14 16:35:18 [INFO] Episode 180\tAvg Length: 169.70\tAvg Reward: -28.111\n",
      "2025-01-14 16:35:19 [INFO] Episode 190\tAvg Length: 97.70\tAvg Reward: -9.812\n",
      "2025-01-14 16:35:21 [INFO] Saved checkpoint at episode 200 -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/saved_models/DDPG_HockeyEnv_BasicOpponent_eps0.1_alr0.0001_clr0.001_gamma0.95_checkpoint_ep200.pth\n",
      "2025-01-14 16:35:21 [INFO] Saved training statistics to -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/stats/DDPG_HockeyEnv_BasicOpponent_eps0.1_alr0.0001_clr0.001_gamma0.95_stats.pkl\n",
      "2025-01-14 16:35:21 [INFO] Episode 200\tAvg Length: 187.60\tAvg Reward: -16.383\n",
      "2025-01-14 16:35:22 [INFO] Episode 210\tAvg Length: 128.60\tAvg Reward: -12.467\n",
      "2025-01-14 16:35:23 [INFO] Episode 220\tAvg Length: 147.30\tAvg Reward: -16.857\n",
      "2025-01-14 16:35:24 [INFO] Episode 230\tAvg Length: 205.50\tAvg Reward: -11.689\n",
      "2025-01-14 16:35:25 [INFO] Episode 240\tAvg Length: 166.70\tAvg Reward: -13.268\n",
      "2025-01-14 16:35:27 [INFO] Episode 250\tAvg Length: 194.50\tAvg Reward: -17.155\n",
      "2025-01-14 16:35:28 [INFO] Episode 260\tAvg Length: 179.60\tAvg Reward: -15.276\n",
      "2025-01-14 16:35:29 [INFO] Episode 270\tAvg Length: 199.80\tAvg Reward: -19.976\n",
      "2025-01-14 16:35:31 [INFO] Episode 280\tAvg Length: 217.10\tAvg Reward: -14.776\n",
      "2025-01-14 16:35:32 [INFO] Episode 290\tAvg Length: 206.10\tAvg Reward: -13.988\n",
      "2025-01-14 16:35:34 [INFO] Saved checkpoint at episode 300 -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/saved_models/DDPG_HockeyEnv_BasicOpponent_eps0.1_alr0.0001_clr0.001_gamma0.95_checkpoint_ep300.pth\n",
      "2025-01-14 16:35:34 [INFO] Saved training statistics to -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/stats/DDPG_HockeyEnv_BasicOpponent_eps0.1_alr0.0001_clr0.001_gamma0.95_stats.pkl\n",
      "2025-01-14 16:35:34 [INFO] Episode 300\tAvg Length: 196.10\tAvg Reward: -13.790\n",
      "2025-01-14 16:35:35 [INFO] Episode 310\tAvg Length: 170.70\tAvg Reward: -12.033\n",
      "2025-01-14 16:35:37 [INFO] Episode 320\tAvg Length: 155.90\tAvg Reward: -14.660\n",
      "2025-01-14 16:35:38 [INFO] Episode 330\tAvg Length: 189.10\tAvg Reward: -16.374\n",
      "2025-01-14 16:35:40 [INFO] Episode 340\tAvg Length: 164.90\tAvg Reward: -14.285\n",
      "2025-01-14 16:35:41 [INFO] Episode 350\tAvg Length: 216.20\tAvg Reward: -13.007\n",
      "2025-01-14 16:35:43 [INFO] Episode 360\tAvg Length: 193.50\tAvg Reward: -11.937\n",
      "2025-01-14 16:35:44 [INFO] Episode 370\tAvg Length: 212.60\tAvg Reward: -15.452\n",
      "2025-01-14 16:35:46 [INFO] Episode 380\tAvg Length: 214.80\tAvg Reward: -12.768\n",
      "2025-01-14 16:35:48 [INFO] Episode 390\tAvg Length: 165.60\tAvg Reward: -14.218\n",
      "2025-01-14 16:35:49 [INFO] Saved checkpoint at episode 400 -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/saved_models/DDPG_HockeyEnv_BasicOpponent_eps0.1_alr0.0001_clr0.001_gamma0.95_checkpoint_ep400.pth\n",
      "2025-01-14 16:35:49 [INFO] Saved training statistics to -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/stats/DDPG_HockeyEnv_BasicOpponent_eps0.1_alr0.0001_clr0.001_gamma0.95_stats.pkl\n",
      "2025-01-14 16:35:49 [INFO] Episode 400\tAvg Length: 172.60\tAvg Reward: -10.819\n",
      "2025-01-14 16:35:51 [INFO] Episode 410\tAvg Length: 198.40\tAvg Reward: -12.264\n",
      "2025-01-14 16:35:53 [INFO] Episode 420\tAvg Length: 171.50\tAvg Reward: -12.758\n",
      "2025-01-14 16:35:54 [INFO] Episode 430\tAvg Length: 209.30\tAvg Reward: -12.143\n",
      "2025-01-14 16:35:56 [INFO] Episode 440\tAvg Length: 175.20\tAvg Reward: -12.369\n",
      "2025-01-14 16:35:58 [INFO] Episode 450\tAvg Length: 209.60\tAvg Reward: -13.162\n",
      "2025-01-14 16:36:00 [INFO] Episode 460\tAvg Length: 151.50\tAvg Reward: -14.597\n",
      "2025-01-14 16:36:02 [INFO] Episode 470\tAvg Length: 189.00\tAvg Reward: -14.806\n",
      "2025-01-14 16:36:03 [INFO] Episode 480\tAvg Length: 184.70\tAvg Reward: -13.398\n",
      "2025-01-14 16:36:05 [INFO] Episode 490\tAvg Length: 155.60\tAvg Reward: -12.470\n",
      "2025-01-14 16:36:07 [INFO] Saved checkpoint at episode 500 -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/saved_models/DDPG_HockeyEnv_BasicOpponent_eps0.1_alr0.0001_clr0.001_gamma0.95_checkpoint_ep500.pth\n",
      "2025-01-14 16:36:07 [INFO] Saved training statistics to -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/stats/DDPG_HockeyEnv_BasicOpponent_eps0.1_alr0.0001_clr0.001_gamma0.95_stats.pkl\n",
      "2025-01-14 16:36:07 [INFO] Episode 500\tAvg Length: 146.40\tAvg Reward: -15.952\n",
      "2025-01-14 16:36:09 [INFO] Episode 510\tAvg Length: 194.50\tAvg Reward: -9.206\n",
      "2025-01-14 16:36:11 [INFO] Episode 520\tAvg Length: 168.50\tAvg Reward: -13.866\n",
      "2025-01-14 16:36:13 [INFO] Episode 530\tAvg Length: 170.40\tAvg Reward: -13.516\n",
      "2025-01-14 16:36:15 [INFO] Episode 540\tAvg Length: 165.10\tAvg Reward: -15.771\n",
      "2025-01-14 16:36:17 [INFO] Episode 550\tAvg Length: 178.70\tAvg Reward: -11.477\n",
      "2025-01-14 16:36:19 [INFO] Episode 560\tAvg Length: 87.60\tAvg Reward: -9.195\n",
      "2025-01-14 16:36:21 [INFO] Episode 570\tAvg Length: 198.80\tAvg Reward: -14.640\n",
      "2025-01-14 16:36:23 [INFO] Episode 580\tAvg Length: 175.10\tAvg Reward: -14.461\n",
      "2025-01-14 16:36:26 [INFO] Episode 590\tAvg Length: 205.50\tAvg Reward: -12.172\n",
      "2025-01-14 16:36:28 [INFO] Saved checkpoint at episode 600 -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/saved_models/DDPG_HockeyEnv_BasicOpponent_eps0.1_alr0.0001_clr0.001_gamma0.95_checkpoint_ep600.pth\n",
      "2025-01-14 16:36:28 [INFO] Saved training statistics to -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/stats/DDPG_HockeyEnv_BasicOpponent_eps0.1_alr0.0001_clr0.001_gamma0.95_stats.pkl\n",
      "2025-01-14 16:36:28 [INFO] Episode 600\tAvg Length: 198.40\tAvg Reward: -13.601\n",
      "2025-01-14 16:36:30 [INFO] Episode 610\tAvg Length: 126.80\tAvg Reward: -9.182\n",
      "2025-01-14 16:36:32 [INFO] Episode 620\tAvg Length: 182.20\tAvg Reward: -8.553\n",
      "2025-01-14 16:36:34 [INFO] Episode 630\tAvg Length: 161.30\tAvg Reward: -13.810\n",
      "2025-01-14 16:36:36 [INFO] Episode 640\tAvg Length: 183.20\tAvg Reward: -13.983\n",
      "2025-01-14 16:36:38 [INFO] Episode 650\tAvg Length: 201.90\tAvg Reward: -10.763\n",
      "2025-01-14 16:36:40 [INFO] Episode 660\tAvg Length: 191.90\tAvg Reward: -12.175\n",
      "2025-01-14 16:36:43 [INFO] Episode 670\tAvg Length: 163.80\tAvg Reward: -11.763\n",
      "2025-01-14 16:36:45 [INFO] Episode 680\tAvg Length: 205.50\tAvg Reward: -11.526\n",
      "2025-01-14 16:36:47 [INFO] Episode 690\tAvg Length: 188.70\tAvg Reward: -11.347\n",
      "2025-01-14 16:36:49 [INFO] Saved checkpoint at episode 700 -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/saved_models/DDPG_HockeyEnv_BasicOpponent_eps0.1_alr0.0001_clr0.001_gamma0.95_checkpoint_ep700.pth\n",
      "2025-01-14 16:36:49 [INFO] Saved training statistics to -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/stats/DDPG_HockeyEnv_BasicOpponent_eps0.1_alr0.0001_clr0.001_gamma0.95_stats.pkl\n",
      "2025-01-14 16:36:49 [INFO] Episode 700\tAvg Length: 193.30\tAvg Reward: -10.517\n",
      "2025-01-14 16:36:52 [INFO] Episode 710\tAvg Length: 162.90\tAvg Reward: -10.105\n",
      "2025-01-14 16:36:54 [INFO] Episode 720\tAvg Length: 196.70\tAvg Reward: -12.925\n",
      "2025-01-14 16:36:56 [INFO] Episode 730\tAvg Length: 204.20\tAvg Reward: -14.297\n",
      "2025-01-14 16:36:58 [INFO] Episode 740\tAvg Length: 144.90\tAvg Reward: -9.274\n",
      "2025-01-14 16:37:00 [INFO] Episode 750\tAvg Length: 182.80\tAvg Reward: -15.840\n",
      "2025-01-14 16:37:02 [INFO] Episode 760\tAvg Length: 170.40\tAvg Reward: -15.654\n",
      "2025-01-14 16:37:04 [INFO] Episode 770\tAvg Length: 173.40\tAvg Reward: -16.887\n",
      "2025-01-14 16:37:07 [INFO] Episode 780\tAvg Length: 184.00\tAvg Reward: -21.920\n",
      "2025-01-14 16:37:09 [INFO] Episode 790\tAvg Length: 170.00\tAvg Reward: -22.301\n",
      "2025-01-14 16:37:11 [INFO] Saved checkpoint at episode 800 -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/saved_models/DDPG_HockeyEnv_BasicOpponent_eps0.1_alr0.0001_clr0.001_gamma0.95_checkpoint_ep800.pth\n",
      "2025-01-14 16:37:11 [INFO] Saved training statistics to -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/stats/DDPG_HockeyEnv_BasicOpponent_eps0.1_alr0.0001_clr0.001_gamma0.95_stats.pkl\n",
      "2025-01-14 16:37:11 [INFO] Episode 800\tAvg Length: 189.00\tAvg Reward: -19.550\n",
      "2025-01-14 16:37:13 [INFO] Episode 810\tAvg Length: 199.40\tAvg Reward: -20.571\n",
      "2025-01-14 16:37:15 [INFO] Episode 820\tAvg Length: 176.70\tAvg Reward: -18.926\n",
      "2025-01-14 16:37:17 [INFO] Episode 830\tAvg Length: 231.00\tAvg Reward: -17.827\n",
      "2025-01-14 16:37:19 [INFO] Episode 840\tAvg Length: 130.10\tAvg Reward: -19.059\n",
      "2025-01-14 16:37:22 [INFO] Episode 850\tAvg Length: 208.00\tAvg Reward: -16.572\n",
      "2025-01-14 16:37:24 [INFO] Episode 860\tAvg Length: 192.90\tAvg Reward: -13.415\n",
      "2025-01-14 16:37:26 [INFO] Episode 870\tAvg Length: 156.00\tAvg Reward: -11.784\n",
      "2025-01-14 16:37:28 [INFO] Episode 880\tAvg Length: 196.80\tAvg Reward: -12.254\n",
      "2025-01-14 16:37:30 [INFO] Episode 890\tAvg Length: 191.70\tAvg Reward: -19.479\n",
      "2025-01-14 16:37:32 [INFO] Saved checkpoint at episode 900 -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/saved_models/DDPG_HockeyEnv_BasicOpponent_eps0.1_alr0.0001_clr0.001_gamma0.95_checkpoint_ep900.pth\n",
      "2025-01-14 16:37:32 [INFO] Saved training statistics to -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/stats/DDPG_HockeyEnv_BasicOpponent_eps0.1_alr0.0001_clr0.001_gamma0.95_stats.pkl\n",
      "2025-01-14 16:37:32 [INFO] Episode 900\tAvg Length: 166.20\tAvg Reward: -18.694\n",
      "2025-01-14 16:37:34 [INFO] Episode 910\tAvg Length: 179.90\tAvg Reward: -13.012\n",
      "2025-01-14 16:37:37 [INFO] Episode 920\tAvg Length: 172.90\tAvg Reward: -19.555\n",
      "2025-01-14 16:37:39 [INFO] Episode 930\tAvg Length: 182.00\tAvg Reward: -19.626\n",
      "2025-01-14 16:37:41 [INFO] Episode 940\tAvg Length: 150.30\tAvg Reward: -14.235\n",
      "2025-01-14 16:37:43 [INFO] Episode 950\tAvg Length: 202.60\tAvg Reward: -15.428\n",
      "2025-01-14 16:37:45 [INFO] Episode 960\tAvg Length: 177.00\tAvg Reward: -10.529\n",
      "2025-01-14 16:37:47 [INFO] Episode 970\tAvg Length: 218.30\tAvg Reward: -19.564\n",
      "2025-01-14 16:37:50 [INFO] Episode 980\tAvg Length: 215.50\tAvg Reward: -17.770\n",
      "2025-01-14 16:37:52 [INFO] Episode 990\tAvg Length: 197.90\tAvg Reward: -15.361\n",
      "2025-01-14 16:37:54 [INFO] Saved checkpoint at episode 1000 -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/saved_models/DDPG_HockeyEnv_BasicOpponent_eps0.1_alr0.0001_clr0.001_gamma0.95_checkpoint_ep1000.pth\n",
      "2025-01-14 16:37:54 [INFO] Saved training statistics to -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/stats/DDPG_HockeyEnv_BasicOpponent_eps0.1_alr0.0001_clr0.001_gamma0.95_stats.pkl\n",
      "2025-01-14 16:37:54 [INFO] Episode 1000\tAvg Length: 169.30\tAvg Reward: -12.412\n",
      "2025-01-14 16:37:56 [INFO] Episode 1010\tAvg Length: 114.00\tAvg Reward: -12.241\n",
      "2025-01-14 16:37:58 [INFO] Episode 1020\tAvg Length: 187.80\tAvg Reward: -13.603\n",
      "2025-01-14 16:38:00 [INFO] Episode 1030\tAvg Length: 164.20\tAvg Reward: -15.386\n",
      "2025-01-14 16:38:02 [INFO] Episode 1040\tAvg Length: 148.80\tAvg Reward: -5.550\n",
      "2025-01-14 16:38:04 [INFO] Episode 1050\tAvg Length: 140.10\tAvg Reward: -10.957\n",
      "2025-01-14 16:38:06 [INFO] Episode 1060\tAvg Length: 150.90\tAvg Reward: -5.429\n",
      "2025-01-14 16:38:08 [INFO] Episode 1070\tAvg Length: 133.90\tAvg Reward: -7.557\n",
      "2025-01-14 16:38:11 [INFO] Episode 1080\tAvg Length: 179.10\tAvg Reward: -8.454\n",
      "2025-01-14 16:38:13 [INFO] Episode 1090\tAvg Length: 156.50\tAvg Reward: -13.705\n",
      "2025-01-14 16:38:15 [INFO] Saved checkpoint at episode 1100 -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/saved_models/DDPG_HockeyEnv_BasicOpponent_eps0.1_alr0.0001_clr0.001_gamma0.95_checkpoint_ep1100.pth\n",
      "2025-01-14 16:38:15 [INFO] Saved training statistics to -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/stats/DDPG_HockeyEnv_BasicOpponent_eps0.1_alr0.0001_clr0.001_gamma0.95_stats.pkl\n",
      "2025-01-14 16:38:15 [INFO] Episode 1100\tAvg Length: 133.30\tAvg Reward: -6.660\n",
      "2025-01-14 16:38:17 [INFO] Episode 1110\tAvg Length: 165.40\tAvg Reward: -11.331\n",
      "2025-01-14 16:38:19 [INFO] Episode 1120\tAvg Length: 154.50\tAvg Reward: -14.869\n",
      "2025-01-14 16:38:21 [INFO] Episode 1130\tAvg Length: 176.00\tAvg Reward: -15.372\n",
      "2025-01-14 16:38:23 [INFO] Episode 1140\tAvg Length: 210.80\tAvg Reward: -14.633\n",
      "2025-01-14 16:38:25 [INFO] Episode 1150\tAvg Length: 138.30\tAvg Reward: -8.549\n",
      "2025-01-14 16:38:28 [INFO] Episode 1160\tAvg Length: 97.40\tAvg Reward: -10.021\n",
      "2025-01-14 16:38:30 [INFO] Episode 1170\tAvg Length: 141.20\tAvg Reward: -14.030\n",
      "2025-01-14 16:38:32 [INFO] Episode 1180\tAvg Length: 149.00\tAvg Reward: -10.062\n",
      "2025-01-14 16:38:34 [INFO] Episode 1190\tAvg Length: 108.80\tAvg Reward: -7.512\n",
      "2025-01-14 16:38:36 [INFO] Saved checkpoint at episode 1200 -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/saved_models/DDPG_HockeyEnv_BasicOpponent_eps0.1_alr0.0001_clr0.001_gamma0.95_checkpoint_ep1200.pth\n",
      "2025-01-14 16:38:36 [INFO] Saved training statistics to -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/stats/DDPG_HockeyEnv_BasicOpponent_eps0.1_alr0.0001_clr0.001_gamma0.95_stats.pkl\n",
      "2025-01-14 16:38:36 [INFO] Episode 1200\tAvg Length: 114.30\tAvg Reward: -10.049\n",
      "2025-01-14 16:38:38 [INFO] Episode 1210\tAvg Length: 140.50\tAvg Reward: -13.998\n",
      "2025-01-14 16:38:40 [INFO] Episode 1220\tAvg Length: 124.70\tAvg Reward: -4.675\n",
      "2025-01-14 16:38:42 [INFO] Episode 1230\tAvg Length: 175.90\tAvg Reward: -6.514\n",
      "2025-01-14 16:38:44 [INFO] Episode 1240\tAvg Length: 101.90\tAvg Reward: -2.358\n",
      "2025-01-14 16:38:46 [INFO] Episode 1250\tAvg Length: 147.10\tAvg Reward: -3.824\n",
      "2025-01-14 16:38:48 [INFO] Episode 1260\tAvg Length: 77.60\tAvg Reward: -7.253\n",
      "2025-01-14 16:38:50 [INFO] Episode 1270\tAvg Length: 84.00\tAvg Reward: -7.168\n",
      "2025-01-14 16:38:52 [INFO] Episode 1280\tAvg Length: 120.10\tAvg Reward: -9.426\n",
      "2025-01-14 16:38:54 [INFO] Episode 1290\tAvg Length: 131.30\tAvg Reward: -15.294\n",
      "2025-01-14 16:38:56 [INFO] Saved checkpoint at episode 1300 -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/saved_models/DDPG_HockeyEnv_BasicOpponent_eps0.1_alr0.0001_clr0.001_gamma0.95_checkpoint_ep1300.pth\n",
      "2025-01-14 16:38:56 [INFO] Saved training statistics to -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/stats/DDPG_HockeyEnv_BasicOpponent_eps0.1_alr0.0001_clr0.001_gamma0.95_stats.pkl\n",
      "2025-01-14 16:38:56 [INFO] Episode 1300\tAvg Length: 99.60\tAvg Reward: -14.014\n",
      "2025-01-14 16:38:58 [INFO] Episode 1310\tAvg Length: 144.90\tAvg Reward: -10.240\n",
      "2025-01-14 16:39:01 [INFO] Episode 1320\tAvg Length: 148.70\tAvg Reward: -10.150\n",
      "2025-01-14 16:39:03 [INFO] Episode 1330\tAvg Length: 153.40\tAvg Reward: -11.959\n",
      "2025-01-14 16:39:05 [INFO] Episode 1340\tAvg Length: 71.60\tAvg Reward: -4.860\n",
      "2025-01-14 16:39:07 [INFO] Episode 1350\tAvg Length: 111.70\tAvg Reward: -2.464\n",
      "2025-01-14 16:39:09 [INFO] Episode 1360\tAvg Length: 124.80\tAvg Reward: -8.460\n",
      "2025-01-14 16:39:11 [INFO] Episode 1370\tAvg Length: 123.90\tAvg Reward: -8.345\n",
      "2025-01-14 16:39:13 [INFO] Episode 1380\tAvg Length: 126.90\tAvg Reward: -3.509\n",
      "2025-01-14 16:39:15 [INFO] Episode 1390\tAvg Length: 177.30\tAvg Reward: -6.812\n",
      "2025-01-14 16:39:17 [INFO] Saved checkpoint at episode 1400 -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/saved_models/DDPG_HockeyEnv_BasicOpponent_eps0.1_alr0.0001_clr0.001_gamma0.95_checkpoint_ep1400.pth\n",
      "2025-01-14 16:39:17 [INFO] Saved training statistics to -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/stats/DDPG_HockeyEnv_BasicOpponent_eps0.1_alr0.0001_clr0.001_gamma0.95_stats.pkl\n",
      "2025-01-14 16:39:17 [INFO] Episode 1400\tAvg Length: 161.10\tAvg Reward: -9.386\n",
      "2025-01-14 16:39:19 [INFO] Episode 1410\tAvg Length: 142.60\tAvg Reward: -3.735\n",
      "2025-01-14 16:39:21 [INFO] Episode 1420\tAvg Length: 119.50\tAvg Reward: -3.455\n",
      "2025-01-14 16:39:24 [INFO] Episode 1430\tAvg Length: 131.70\tAvg Reward: -7.590\n",
      "2025-01-14 16:39:26 [INFO] Episode 1440\tAvg Length: 171.00\tAvg Reward: -6.399\n",
      "2025-01-14 16:39:28 [INFO] Episode 1450\tAvg Length: 149.90\tAvg Reward: -6.682\n",
      "2025-01-14 16:39:30 [INFO] Episode 1460\tAvg Length: 148.40\tAvg Reward: -10.049\n",
      "2025-01-14 16:39:32 [INFO] Episode 1470\tAvg Length: 165.50\tAvg Reward: -8.359\n",
      "2025-01-14 16:39:34 [INFO] Episode 1480\tAvg Length: 162.60\tAvg Reward: -11.153\n",
      "2025-01-14 16:39:36 [INFO] Episode 1490\tAvg Length: 153.80\tAvg Reward: -11.452\n",
      "2025-01-14 16:39:38 [INFO] Saved checkpoint at episode 1500 -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/saved_models/DDPG_HockeyEnv_BasicOpponent_eps0.1_alr0.0001_clr0.001_gamma0.95_checkpoint_ep1500.pth\n",
      "2025-01-14 16:39:38 [INFO] Saved training statistics to -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/stats/DDPG_HockeyEnv_BasicOpponent_eps0.1_alr0.0001_clr0.001_gamma0.95_stats.pkl\n",
      "2025-01-14 16:39:38 [INFO] Episode 1500\tAvg Length: 203.00\tAvg Reward: -8.363\n",
      "2025-01-14 16:39:40 [INFO] Episode 1510\tAvg Length: 122.70\tAvg Reward: -8.132\n",
      "2025-01-14 16:39:43 [INFO] Episode 1520\tAvg Length: 150.00\tAvg Reward: -9.859\n",
      "2025-01-14 16:39:45 [INFO] Episode 1530\tAvg Length: 156.20\tAvg Reward: -8.358\n",
      "2025-01-14 16:39:47 [INFO] Episode 1540\tAvg Length: 160.30\tAvg Reward: -11.933\n",
      "2025-01-14 16:39:49 [INFO] Episode 1550\tAvg Length: 196.10\tAvg Reward: -15.656\n",
      "2025-01-14 16:39:51 [INFO] Episode 1560\tAvg Length: 155.30\tAvg Reward: -10.395\n",
      "2025-01-14 16:39:53 [INFO] Episode 1570\tAvg Length: 140.30\tAvg Reward: -9.014\n",
      "2025-01-14 16:39:55 [INFO] Episode 1580\tAvg Length: 158.20\tAvg Reward: -10.163\n",
      "2025-01-14 16:39:58 [INFO] Episode 1590\tAvg Length: 141.50\tAvg Reward: -13.586\n",
      "2025-01-14 16:40:00 [INFO] Saved checkpoint at episode 1600 -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/saved_models/DDPG_HockeyEnv_BasicOpponent_eps0.1_alr0.0001_clr0.001_gamma0.95_checkpoint_ep1600.pth\n",
      "2025-01-14 16:40:00 [INFO] Saved training statistics to -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/stats/DDPG_HockeyEnv_BasicOpponent_eps0.1_alr0.0001_clr0.001_gamma0.95_stats.pkl\n",
      "2025-01-14 16:40:00 [INFO] Episode 1600\tAvg Length: 71.70\tAvg Reward: -3.471\n",
      "2025-01-14 16:40:02 [INFO] Episode 1610\tAvg Length: 132.80\tAvg Reward: -6.663\n",
      "2025-01-14 16:40:04 [INFO] Episode 1620\tAvg Length: 96.10\tAvg Reward: -7.895\n",
      "2025-01-14 16:40:06 [INFO] Episode 1630\tAvg Length: 97.00\tAvg Reward: -14.005\n",
      "2025-01-14 16:40:08 [INFO] Episode 1640\tAvg Length: 83.80\tAvg Reward: -11.032\n",
      "2025-01-14 16:40:10 [INFO] Episode 1650\tAvg Length: 145.90\tAvg Reward: -8.471\n",
      "2025-01-14 16:40:12 [INFO] Episode 1660\tAvg Length: 145.60\tAvg Reward: -4.438\n",
      "2025-01-14 16:40:14 [INFO] Episode 1670\tAvg Length: 102.90\tAvg Reward: -9.594\n",
      "2025-01-14 16:40:16 [INFO] Episode 1680\tAvg Length: 72.70\tAvg Reward: -1.784\n",
      "2025-01-14 16:40:18 [INFO] Episode 1690\tAvg Length: 105.50\tAvg Reward: -7.918\n",
      "2025-01-14 16:40:20 [INFO] Saved checkpoint at episode 1700 -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/saved_models/DDPG_HockeyEnv_BasicOpponent_eps0.1_alr0.0001_clr0.001_gamma0.95_checkpoint_ep1700.pth\n",
      "2025-01-14 16:40:20 [INFO] Saved training statistics to -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/stats/DDPG_HockeyEnv_BasicOpponent_eps0.1_alr0.0001_clr0.001_gamma0.95_stats.pkl\n",
      "2025-01-14 16:40:20 [INFO] Episode 1700\tAvg Length: 129.60\tAvg Reward: -11.568\n",
      "2025-01-14 16:40:22 [INFO] Episode 1710\tAvg Length: 93.00\tAvg Reward: -6.998\n",
      "2025-01-14 16:40:24 [INFO] Episode 1720\tAvg Length: 152.40\tAvg Reward: -5.571\n",
      "2025-01-14 16:40:26 [INFO] Episode 1730\tAvg Length: 128.60\tAvg Reward: -10.669\n",
      "2025-01-14 16:40:28 [INFO] Episode 1740\tAvg Length: 109.70\tAvg Reward: -0.396\n",
      "2025-01-14 16:40:30 [INFO] Episode 1750\tAvg Length: 116.30\tAvg Reward: -11.680\n",
      "2025-01-14 16:40:32 [INFO] Episode 1760\tAvg Length: 96.90\tAvg Reward: -3.461\n",
      "2025-01-14 16:40:34 [INFO] Episode 1770\tAvg Length: 115.80\tAvg Reward: -10.878\n",
      "2025-01-14 16:40:36 [INFO] Episode 1780\tAvg Length: 99.00\tAvg Reward: -3.951\n",
      "2025-01-14 16:40:38 [INFO] Episode 1790\tAvg Length: 114.30\tAvg Reward: -6.944\n",
      "2025-01-14 16:40:40 [INFO] Saved checkpoint at episode 1800 -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/saved_models/DDPG_HockeyEnv_BasicOpponent_eps0.1_alr0.0001_clr0.001_gamma0.95_checkpoint_ep1800.pth\n",
      "2025-01-14 16:40:40 [INFO] Saved training statistics to -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/stats/DDPG_HockeyEnv_BasicOpponent_eps0.1_alr0.0001_clr0.001_gamma0.95_stats.pkl\n",
      "2025-01-14 16:40:40 [INFO] Episode 1800\tAvg Length: 91.70\tAvg Reward: -6.051\n",
      "2025-01-14 16:40:43 [INFO] Episode 1810\tAvg Length: 152.50\tAvg Reward: -5.742\n",
      "2025-01-14 16:40:45 [INFO] Episode 1820\tAvg Length: 99.30\tAvg Reward: -4.578\n",
      "2025-01-14 16:40:47 [INFO] Episode 1830\tAvg Length: 130.40\tAvg Reward: -4.056\n",
      "2025-01-14 16:40:49 [INFO] Episode 1840\tAvg Length: 132.30\tAvg Reward: -7.750\n",
      "2025-01-14 16:40:51 [INFO] Episode 1850\tAvg Length: 125.70\tAvg Reward: -9.407\n",
      "2025-01-14 16:40:53 [INFO] Episode 1860\tAvg Length: 151.20\tAvg Reward: -7.660\n",
      "2025-01-14 16:40:55 [INFO] Episode 1870\tAvg Length: 75.80\tAvg Reward: -5.100\n",
      "2025-01-14 16:40:57 [INFO] Episode 1880\tAvg Length: 144.70\tAvg Reward: -4.741\n",
      "2025-01-14 16:40:59 [INFO] Episode 1890\tAvg Length: 90.90\tAvg Reward: -7.120\n",
      "2025-01-14 16:41:01 [INFO] Saved checkpoint at episode 1900 -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/saved_models/DDPG_HockeyEnv_BasicOpponent_eps0.1_alr0.0001_clr0.001_gamma0.95_checkpoint_ep1900.pth\n",
      "2025-01-14 16:41:01 [INFO] Saved training statistics to -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/stats/DDPG_HockeyEnv_BasicOpponent_eps0.1_alr0.0001_clr0.001_gamma0.95_stats.pkl\n",
      "2025-01-14 16:41:01 [INFO] Episode 1900\tAvg Length: 116.80\tAvg Reward: -7.724\n",
      "2025-01-14 16:41:03 [INFO] Episode 1910\tAvg Length: 104.00\tAvg Reward: -5.963\n",
      "2025-01-14 16:41:05 [INFO] Episode 1920\tAvg Length: 158.10\tAvg Reward: -13.057\n",
      "2025-01-14 16:41:07 [INFO] Episode 1930\tAvg Length: 162.10\tAvg Reward: -9.780\n",
      "2025-01-14 16:41:09 [INFO] Episode 1940\tAvg Length: 113.70\tAvg Reward: -11.410\n",
      "2025-01-14 16:41:11 [INFO] Episode 1950\tAvg Length: 120.80\tAvg Reward: -7.417\n",
      "2025-01-14 16:41:13 [INFO] Episode 1960\tAvg Length: 126.60\tAvg Reward: -4.003\n",
      "2025-01-14 16:41:15 [INFO] Episode 1970\tAvg Length: 109.30\tAvg Reward: -1.607\n",
      "2025-01-14 16:41:17 [INFO] Episode 1980\tAvg Length: 108.50\tAvg Reward: -9.906\n",
      "2025-01-14 16:41:19 [INFO] Episode 1990\tAvg Length: 159.20\tAvg Reward: -7.288\n",
      "2025-01-14 16:41:21 [INFO] Saved checkpoint at episode 2000 -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/saved_models/DDPG_HockeyEnv_BasicOpponent_eps0.1_alr0.0001_clr0.001_gamma0.95_checkpoint_ep2000.pth\n",
      "2025-01-14 16:41:21 [INFO] Saved training statistics to -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/stats/DDPG_HockeyEnv_BasicOpponent_eps0.1_alr0.0001_clr0.001_gamma0.95_stats.pkl\n",
      "2025-01-14 16:41:21 [INFO] Episode 2000\tAvg Length: 92.70\tAvg Reward: -5.412\n",
      "2025-01-14 16:41:21 [INFO] Saved training statistics to -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/stats/DDPG_HockeyEnv_BasicOpponent_eps0.1_alr0.0001_clr0.001_gamma0.95_stats.pkl\n",
      "2025-01-14 16:41:22 [INFO] Final training metrics: {'average_reward': -12.253447021747442, 'average_length': 156.2565, 'final_loss_critic': 0.0944790318608284, 'final_loss_actor': 0.4990100562572479}\n",
      "Training finished.\n",
      "Final metrics: {'average_reward': -12.253447021747442, 'average_length': 156.2565, 'final_loss_critic': 0.0944790318608284, 'final_loss_actor': 0.4990100562572479}\n"
     ]
    }
   ],
   "source": [
    "# --- 5. Train the Agent ---\n",
    "\n",
    "final_metrics = trainer.train()\n",
    "\n",
    "print(\"Training finished.\")\n",
    "print(\"Final metrics:\", final_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x2/dvcm78hn67g79fdt399dzt8c0000gn/T/ipykernel_23582/4061101739.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "checkpoint_path = \"rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/saved_models/DDPG_HockeyEnv_BasicOpponent_eps0.1_alr0.0001_clr0.001_gamma0.99_checkpoint_ep2000.pth\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "\n",
    "# 4) Restore the agent's networks\n",
    "trainer.agent.restore_state(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Episode 1, Reward: -10.77\n",
      "Test Episode 2, Reward: 7.07\n",
      "Test Episode 3, Reward: -12.45\n",
      "Test Episode 4, Reward: -13.12\n",
      "Test Episode 5, Reward: -10.02\n",
      "Test Episode 6, Reward: 9.27\n",
      "Test Episode 7, Reward: -13.03\n",
      "Test Episode 8, Reward: -34.52\n",
      "Test Episode 9, Reward: -11.60\n",
      "Test Episode 10, Reward: -3.28\n"
     ]
    }
   ],
   "source": [
    "# --- 6. Testing / Demo (Optional) ---\n",
    "\n",
    "# If you want to watch a few episodes:\n",
    "num_test_episodes = 10\n",
    "for ep in range(num_test_episodes):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0.0\n",
    "    while not done:\n",
    "        # Simple deterministic policy (no noise)\n",
    "        action = trainer.agent.act(obs, eps=0.0)  # zero noise\n",
    "        obs, reward, done, trunc, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "        # If you want to watch the environment\n",
    "        env.render(mode=\"rgb_array\")\n",
    "\n",
    "        if done or trunc:\n",
    "            print(f\"Test Episode {ep+1}, Reward: {episode_reward:.2f}\")\n",
    "            break\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode reward: -12.310578733653294\n",
      "Saved GIF to ddpg_laserhockey_episode0.gif\n",
      "Episode reward: -22.159581652821615\n",
      "Saved GIF to ddpg_laserhockey_episode1.gif\n",
      "Episode reward: -8.00500476368653\n",
      "Saved GIF to ddpg_laserhockey_episode2.gif\n",
      "Episode reward: -16.056604594272734\n",
      "Saved GIF to ddpg_laserhockey_episode3.gif\n",
      "Episode reward: -8.771734848460094\n",
      "Saved GIF to ddpg_laserhockey_episode4.gif\n",
      "Episode reward: -8.991283737925658\n",
      "Saved GIF to ddpg_laserhockey_episode5.gif\n",
      "Episode reward: -2.0722434075932195\n",
      "Saved GIF to ddpg_laserhockey_episode6.gif\n",
      "Episode reward: -13.641432833137229\n",
      "Saved GIF to ddpg_laserhockey_episode7.gif\n",
      "Episode reward: -4.937669097315655\n",
      "Saved GIF to ddpg_laserhockey_episode8.gif\n",
      "Episode reward: -7.79536010486082\n",
      "Saved GIF to ddpg_laserhockey_episode9.gif\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "num_test_episodes = 10\n",
    "for ep in range(num_test_episodes):\n",
    "    frames = []\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    trainer.agent.reset()\n",
    "\n",
    "\n",
    "    episode_reward = 0.0\n",
    "    while not done:\n",
    "        # Act with no noise at test time\n",
    "        action = trainer.agent.act(obs, eps=0.0)\n",
    "        obs, reward, done, trunc, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "        # ---- Capture frame in rgb_array mode ----\n",
    "        frame_rgb = env.render(mode='rgb_array')  \n",
    "        frames.append(frame_rgb)\n",
    "\n",
    "        if done or trunc:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    print(f\"Episode reward: {episode_reward}\")\n",
    "\n",
    "    # 4) Save frames as GIF\n",
    "    gif_path = f\"ddpg_laserhockey_episode{ep}.gif\"\n",
    "    imageio.mimsave(gif_path, frames, fps=15)  # set fps as desired\n",
    "    print(f\"Saved GIF to {gif_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GIF to ddpg_laserhockey_episode.gif\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4) Save frames as GIF\n",
    "gif_path = \"ddpg_laserhockey_episode.gif\"\n",
    "imageio.mimsave(gif_path, frames, fps=15)  # set fps as desired\n",
    "print(f\"Saved GIF to {gif_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
