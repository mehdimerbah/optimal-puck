{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# sys.path.append(\"/path/to/your/project\")\n",
    "from hockey.hockey_env import HockeyEnv_BasicOpponent, Mode, HockeyEnv\n",
    "\n",
    "try:\n",
    "    from models.ddpg.DDPG import DDPGAgent\n",
    "    from models.ddpg.DDPGTrainer import DDPGTrainer\n",
    "except ImportError:\n",
    "    print(\"Could not import DDPG.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box(-inf, inf, (18,), float32)\n",
      "Action space: Box(-1.0, 1.0, (4,), float32)\n"
     ]
    }
   ],
   "source": [
    "# The single-agent wrapper environment is `HockeyEnv_BasicOpponent`.\n",
    "# By default:\n",
    "#   - It uses mode=Mode.NORMAL\n",
    "#   - keep_mode=True\n",
    "#   - An opponent with `weak_opponent=False` or True\n",
    "#\n",
    "# action_space of shape (4,) \n",
    "# suitable for a single-agent continuous control algorithms\n",
    "\n",
    "env = HockeyEnv_BasicOpponent(\n",
    "    mode=Mode.NORMAL,   # or Mode.TRAIN_SHOOTING, Mode.TRAIN_DEFENSE\n",
    "    weak_opponent=True # whether the opponent is weaker or not\n",
    ")\n",
    "\n",
    "print(\"Observation space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)\n",
    "\n",
    "# For reference:\n",
    "# - The observation space is Box(...) with shape (18,) if keep_mode=True.\n",
    "# - The action space is Box(...) with shape (4,). \n",
    "#   The four actions: \n",
    "#       1) Force in x, \n",
    "#       2) Force in y, \n",
    "#       3) Torque (racket rotation),\n",
    "#       4) Shoot command (0 or 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box(-inf, inf, (18,), float32)\n",
      "Action space: Box(-1.0, 1.0, (8,), float32)\n"
     ]
    }
   ],
   "source": [
    "self_play_env = HockeyEnv(mode=Mode.NORMAL, keep_mode=True)\n",
    "print(\"Observation space:\", self_play_env.observation_space)\n",
    "print(\"Action space:\", self_play_env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-29 19:22:05 [INFO] Logger initialized. Writing logs to rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/logs/DDPG_HockeyEnv_noise0.1_alr0.0001_clr0.0001_gamma0.95.log\n",
      "2025-01-29 19:22:05 [INFO] Initialized random seeds to 42.\n"
     ]
    }
   ],
   "source": [
    "training_config = {\n",
    "    \"env_mode\": Mode.NORMAL,\n",
    "    \"max_episodes\": 10000,     \n",
    "    \"max_timesteps\": 250,    \n",
    "    \"log_interval\": 20,\n",
    "    \"save_interval\": 1000,\n",
    "    \"render\": False,         \n",
    "    \"train_iter\": 32,       \n",
    "    \"seed\": 42      \n",
    "}\n",
    "\n",
    "model_config = {\n",
    "    \"noise_scale\": 0.1,                  \n",
    "    \"discount\": 0.95,      \n",
    "    \"buffer_size\": int(1e6),     \n",
    "    \"batch_size\": 128,            \n",
    "    \"learning_rate_actor\": 1e-4, \n",
    "    \"learning_rate_critic\": 1e-4,\n",
    "    \"hidden_sizes_actor\": [256, 256],\n",
    "    \"hidden_sizes_critic\": [256, 256],\n",
    "    \"update_target_every\": 100,\n",
    "    \"use_target_net\": True\n",
    "}\n",
    "\n",
    "experiment_path = \"rl_experiments/experiments/HockeyEnv_DDPG_Test\"\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = DDPGTrainer(\n",
    "    env_name=\"HockeyEnv\",\n",
    "    training_config=training_config,\n",
    "    model_config=model_config,\n",
    "    experiment_path=experiment_path,\n",
    "    wandb_run=None \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x2/dvcm78hn67g79fdt399dzt8c0000gn/T/ipykernel_4447/2494310159.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"DDPG_HockeyEnv_noise0.1_alr0.0001_clr0.0001_gamma0.95_checkpoint_ep18000.pth\"\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "\n",
    "# restore the agent's networks\n",
    "trainer.agent.restore_state(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-26 23:32:01 [INFO] Starting DDPG Training...\n",
      "2025-01-26 23:32:01 [INFO] Environment: HockeyEnv, max_episodes=10000, max_timesteps=250, train_iter=32\n",
      "2025-01-26 23:32:03 [INFO] Episode 20\tAvg Length: 149.00\tAvg Reward: -8.917\n",
      "2025-01-26 23:32:05 [INFO] Episode 40\tAvg Length: 161.05\tAvg Reward: -9.821\n",
      "2025-01-26 23:32:07 [INFO] Episode 60\tAvg Length: 162.90\tAvg Reward: -12.780\n",
      "2025-01-26 23:32:10 [INFO] Episode 80\tAvg Length: 142.25\tAvg Reward: -7.654\n",
      "2025-01-26 23:32:12 [INFO] Episode 100\tAvg Length: 135.85\tAvg Reward: -9.985\n",
      "2025-01-26 23:32:14 [INFO] Episode 120\tAvg Length: 143.60\tAvg Reward: -9.728\n",
      "2025-01-26 23:32:17 [INFO] Episode 140\tAvg Length: 146.50\tAvg Reward: -10.438\n",
      "2025-01-26 23:32:19 [INFO] Episode 160\tAvg Length: 164.05\tAvg Reward: -8.222\n",
      "2025-01-26 23:32:22 [INFO] Episode 180\tAvg Length: 131.05\tAvg Reward: -6.618\n",
      "2025-01-26 23:32:25 [INFO] Episode 200\tAvg Length: 125.65\tAvg Reward: -9.051\n",
      "2025-01-26 23:32:27 [INFO] Episode 220\tAvg Length: 145.95\tAvg Reward: -5.985\n",
      "2025-01-26 23:32:30 [INFO] Episode 240\tAvg Length: 180.30\tAvg Reward: -11.235\n",
      "2025-01-26 23:32:33 [INFO] Episode 260\tAvg Length: 139.30\tAvg Reward: -6.041\n",
      "2025-01-26 23:32:37 [INFO] Episode 280\tAvg Length: 178.10\tAvg Reward: -11.485\n",
      "2025-01-26 23:32:40 [INFO] Episode 300\tAvg Length: 119.25\tAvg Reward: -11.064\n",
      "2025-01-26 23:32:43 [INFO] Episode 320\tAvg Length: 152.10\tAvg Reward: -9.306\n",
      "2025-01-26 23:32:47 [INFO] Episode 340\tAvg Length: 182.60\tAvg Reward: -12.051\n",
      "2025-01-26 23:32:50 [INFO] Episode 360\tAvg Length: 192.30\tAvg Reward: -10.594\n",
      "2025-01-26 23:32:54 [INFO] Episode 380\tAvg Length: 138.20\tAvg Reward: -11.408\n",
      "2025-01-26 23:32:57 [INFO] Episode 400\tAvg Length: 106.60\tAvg Reward: -4.505\n",
      "2025-01-26 23:33:01 [INFO] Episode 420\tAvg Length: 176.05\tAvg Reward: -13.211\n",
      "2025-01-26 23:33:05 [INFO] Episode 440\tAvg Length: 159.15\tAvg Reward: -8.730\n",
      "2025-01-26 23:33:09 [INFO] Episode 460\tAvg Length: 154.30\tAvg Reward: -7.422\n",
      "2025-01-26 23:33:13 [INFO] Episode 480\tAvg Length: 177.70\tAvg Reward: -7.148\n",
      "2025-01-26 23:33:17 [INFO] Episode 500\tAvg Length: 182.10\tAvg Reward: -9.060\n",
      "2025-01-26 23:33:21 [INFO] Episode 520\tAvg Length: 120.15\tAvg Reward: -6.330\n",
      "2025-01-26 23:33:26 [INFO] Episode 540\tAvg Length: 120.60\tAvg Reward: -5.783\n",
      "2025-01-26 23:33:31 [INFO] Episode 560\tAvg Length: 145.65\tAvg Reward: -8.103\n",
      "2025-01-26 23:33:35 [INFO] Episode 580\tAvg Length: 133.40\tAvg Reward: -8.444\n",
      "2025-01-26 23:33:40 [INFO] Episode 600\tAvg Length: 154.40\tAvg Reward: -9.808\n",
      "2025-01-26 23:33:45 [INFO] Episode 620\tAvg Length: 151.70\tAvg Reward: -15.194\n",
      "2025-01-26 23:33:50 [INFO] Episode 640\tAvg Length: 162.70\tAvg Reward: -15.369\n",
      "2025-01-26 23:33:54 [INFO] Episode 660\tAvg Length: 162.50\tAvg Reward: -10.077\n",
      "2025-01-26 23:33:59 [INFO] Episode 680\tAvg Length: 161.70\tAvg Reward: -9.769\n",
      "2025-01-26 23:34:04 [INFO] Episode 700\tAvg Length: 162.55\tAvg Reward: -15.803\n",
      "2025-01-26 23:34:09 [INFO] Episode 720\tAvg Length: 129.00\tAvg Reward: -7.445\n",
      "2025-01-26 23:34:14 [INFO] Episode 740\tAvg Length: 124.85\tAvg Reward: -3.250\n",
      "2025-01-26 23:34:20 [INFO] Episode 760\tAvg Length: 157.60\tAvg Reward: -10.255\n",
      "2025-01-26 23:34:25 [INFO] Episode 780\tAvg Length: 143.80\tAvg Reward: -5.407\n",
      "2025-01-26 23:34:30 [INFO] Episode 800\tAvg Length: 126.05\tAvg Reward: -3.724\n",
      "2025-01-26 23:34:36 [INFO] Episode 820\tAvg Length: 149.80\tAvg Reward: -6.284\n",
      "2025-01-26 23:34:42 [INFO] Episode 840\tAvg Length: 167.95\tAvg Reward: -7.256\n",
      "2025-01-26 23:34:48 [INFO] Episode 860\tAvg Length: 177.65\tAvg Reward: -9.446\n",
      "2025-01-26 23:34:54 [INFO] Episode 880\tAvg Length: 170.25\tAvg Reward: -8.327\n",
      "2025-01-26 23:35:00 [INFO] Episode 900\tAvg Length: 129.20\tAvg Reward: -1.389\n",
      "2025-01-26 23:35:06 [INFO] Episode 920\tAvg Length: 125.80\tAvg Reward: -4.169\n",
      "2025-01-26 23:35:12 [INFO] Episode 940\tAvg Length: 211.95\tAvg Reward: -11.562\n",
      "2025-01-26 23:35:18 [INFO] Episode 960\tAvg Length: 175.50\tAvg Reward: -4.085\n",
      "2025-01-26 23:35:25 [INFO] Episode 980\tAvg Length: 139.90\tAvg Reward: -7.600\n",
      "2025-01-26 23:35:31 [INFO] Saved checkpoint at episode 1000 -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/saved_models/DDPG_HockeyEnv_noise0.1_alr0.0001_clr0.0001_gamma0.95_checkpoint_ep1000.pth\n",
      "2025-01-26 23:35:31 [INFO] Saved training statistics to -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/stats/DDPG_HockeyEnv_noise0.1_alr0.0001_clr0.0001_gamma0.95_stats.pkl\n",
      "2025-01-26 23:35:31 [INFO] Episode 1000\tAvg Length: 181.00\tAvg Reward: -4.621\n",
      "2025-01-26 23:35:38 [INFO] Episode 1020\tAvg Length: 183.85\tAvg Reward: -8.168\n",
      "2025-01-26 23:35:45 [INFO] Episode 1040\tAvg Length: 159.55\tAvg Reward: -7.981\n",
      "2025-01-26 23:35:52 [INFO] Episode 1060\tAvg Length: 152.45\tAvg Reward: -8.821\n",
      "2025-01-26 23:35:59 [INFO] Episode 1080\tAvg Length: 157.35\tAvg Reward: -8.983\n",
      "2025-01-26 23:36:06 [INFO] Episode 1100\tAvg Length: 167.00\tAvg Reward: -8.884\n",
      "2025-01-26 23:36:13 [INFO] Episode 1120\tAvg Length: 180.45\tAvg Reward: -11.024\n",
      "2025-01-26 23:36:21 [INFO] Episode 1140\tAvg Length: 147.75\tAvg Reward: -4.904\n",
      "2025-01-26 23:36:28 [INFO] Episode 1160\tAvg Length: 191.95\tAvg Reward: -8.517\n",
      "2025-01-26 23:36:36 [INFO] Episode 1180\tAvg Length: 184.75\tAvg Reward: -10.085\n",
      "2025-01-26 23:36:43 [INFO] Episode 1200\tAvg Length: 196.25\tAvg Reward: -10.336\n",
      "2025-01-26 23:36:51 [INFO] Episode 1220\tAvg Length: 137.70\tAvg Reward: -6.564\n",
      "2025-01-26 23:36:58 [INFO] Episode 1240\tAvg Length: 161.95\tAvg Reward: -5.383\n",
      "2025-01-26 23:37:06 [INFO] Episode 1260\tAvg Length: 129.30\tAvg Reward: -4.843\n",
      "2025-01-26 23:37:14 [INFO] Episode 1280\tAvg Length: 141.05\tAvg Reward: -6.026\n",
      "2025-01-26 23:37:22 [INFO] Episode 1300\tAvg Length: 139.25\tAvg Reward: -8.014\n",
      "2025-01-26 23:37:30 [INFO] Episode 1320\tAvg Length: 153.50\tAvg Reward: -4.493\n",
      "2025-01-26 23:37:38 [INFO] Episode 1340\tAvg Length: 169.15\tAvg Reward: -6.800\n",
      "2025-01-26 23:37:47 [INFO] Episode 1360\tAvg Length: 142.35\tAvg Reward: -12.265\n",
      "2025-01-26 23:37:55 [INFO] Episode 1380\tAvg Length: 130.30\tAvg Reward: -3.501\n",
      "2025-01-26 23:38:03 [INFO] Episode 1400\tAvg Length: 139.85\tAvg Reward: -6.503\n",
      "2025-01-26 23:38:13 [INFO] Episode 1420\tAvg Length: 194.40\tAvg Reward: -9.011\n",
      "2025-01-26 23:38:22 [INFO] Episode 1440\tAvg Length: 118.55\tAvg Reward: -0.873\n",
      "2025-01-26 23:38:31 [INFO] Episode 1460\tAvg Length: 180.20\tAvg Reward: -10.386\n",
      "2025-01-26 23:38:40 [INFO] Episode 1480\tAvg Length: 110.05\tAvg Reward: -2.135\n",
      "2025-01-26 23:38:48 [INFO] Episode 1500\tAvg Length: 129.90\tAvg Reward: -5.740\n",
      "2025-01-26 23:38:57 [INFO] Episode 1520\tAvg Length: 167.10\tAvg Reward: -6.884\n",
      "2025-01-26 23:39:06 [INFO] Episode 1540\tAvg Length: 141.05\tAvg Reward: -6.988\n",
      "2025-01-26 23:39:16 [INFO] Episode 1560\tAvg Length: 164.60\tAvg Reward: -6.795\n",
      "2025-01-26 23:39:25 [INFO] Episode 1580\tAvg Length: 96.25\tAvg Reward: -4.211\n",
      "2025-01-26 23:39:34 [INFO] Episode 1600\tAvg Length: 137.75\tAvg Reward: -5.898\n",
      "2025-01-26 23:39:43 [INFO] Episode 1620\tAvg Length: 129.10\tAvg Reward: -7.315\n",
      "2025-01-26 23:39:52 [INFO] Episode 1640\tAvg Length: 133.65\tAvg Reward: -5.338\n",
      "2025-01-26 23:40:02 [INFO] Episode 1660\tAvg Length: 152.00\tAvg Reward: -6.104\n",
      "2025-01-26 23:40:11 [INFO] Episode 1680\tAvg Length: 152.10\tAvg Reward: -2.530\n",
      "2025-01-26 23:40:21 [INFO] Episode 1700\tAvg Length: 162.75\tAvg Reward: -9.905\n",
      "2025-01-26 23:40:31 [INFO] Episode 1720\tAvg Length: 117.25\tAvg Reward: -3.753\n",
      "2025-01-26 23:40:41 [INFO] Episode 1740\tAvg Length: 156.40\tAvg Reward: -5.011\n",
      "2025-01-26 23:40:51 [INFO] Episode 1760\tAvg Length: 136.35\tAvg Reward: -6.882\n",
      "2025-01-26 23:41:01 [INFO] Episode 1780\tAvg Length: 140.55\tAvg Reward: -5.967\n",
      "2025-01-26 23:41:12 [INFO] Episode 1800\tAvg Length: 170.65\tAvg Reward: -4.757\n",
      "2025-01-26 23:41:22 [INFO] Episode 1820\tAvg Length: 162.05\tAvg Reward: -10.450\n",
      "2025-01-26 23:41:33 [INFO] Episode 1840\tAvg Length: 145.15\tAvg Reward: -9.712\n",
      "2025-01-26 23:41:43 [INFO] Episode 1860\tAvg Length: 153.65\tAvg Reward: -8.767\n",
      "2025-01-26 23:41:54 [INFO] Episode 1880\tAvg Length: 131.65\tAvg Reward: -4.847\n",
      "2025-01-26 23:42:05 [INFO] Episode 1900\tAvg Length: 147.90\tAvg Reward: -8.855\n",
      "2025-01-26 23:42:16 [INFO] Episode 1920\tAvg Length: 144.40\tAvg Reward: -6.837\n",
      "2025-01-26 23:42:27 [INFO] Episode 1940\tAvg Length: 191.75\tAvg Reward: -12.201\n",
      "2025-01-26 23:42:38 [INFO] Episode 1960\tAvg Length: 140.30\tAvg Reward: -7.853\n",
      "2025-01-26 23:42:48 [INFO] Episode 1980\tAvg Length: 188.45\tAvg Reward: -11.250\n",
      "2025-01-26 23:42:59 [INFO] Saved checkpoint at episode 2000 -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/saved_models/DDPG_HockeyEnv_noise0.1_alr0.0001_clr0.0001_gamma0.95_checkpoint_ep2000.pth\n",
      "2025-01-26 23:42:59 [INFO] Saved training statistics to -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/stats/DDPG_HockeyEnv_noise0.1_alr0.0001_clr0.0001_gamma0.95_stats.pkl\n",
      "2025-01-26 23:42:59 [INFO] Episode 2000\tAvg Length: 133.20\tAvg Reward: -3.387\n",
      "2025-01-26 23:43:10 [INFO] Episode 2020\tAvg Length: 149.15\tAvg Reward: -7.252\n",
      "2025-01-26 23:43:20 [INFO] Episode 2040\tAvg Length: 164.75\tAvg Reward: -7.189\n",
      "2025-01-26 23:43:31 [INFO] Episode 2060\tAvg Length: 175.80\tAvg Reward: -9.816\n",
      "2025-01-26 23:43:42 [INFO] Episode 2080\tAvg Length: 161.25\tAvg Reward: -5.607\n",
      "2025-01-26 23:43:53 [INFO] Episode 2100\tAvg Length: 156.50\tAvg Reward: -6.841\n",
      "2025-01-26 23:44:05 [INFO] Episode 2120\tAvg Length: 175.30\tAvg Reward: -4.314\n",
      "2025-01-26 23:44:16 [INFO] Episode 2140\tAvg Length: 140.55\tAvg Reward: -3.141\n",
      "2025-01-26 23:44:27 [INFO] Episode 2160\tAvg Length: 138.20\tAvg Reward: -4.165\n",
      "2025-01-26 23:44:38 [INFO] Episode 2180\tAvg Length: 174.80\tAvg Reward: -6.069\n",
      "2025-01-26 23:44:50 [INFO] Episode 2200\tAvg Length: 152.70\tAvg Reward: -5.175\n",
      "2025-01-26 23:45:01 [INFO] Episode 2220\tAvg Length: 114.30\tAvg Reward: -3.188\n",
      "2025-01-26 23:45:13 [INFO] Episode 2240\tAvg Length: 150.90\tAvg Reward: -5.990\n",
      "2025-01-26 23:45:25 [INFO] Episode 2260\tAvg Length: 176.45\tAvg Reward: -6.514\n",
      "2025-01-26 23:45:36 [INFO] Episode 2280\tAvg Length: 148.45\tAvg Reward: -9.702\n",
      "2025-01-26 23:45:49 [INFO] Episode 2300\tAvg Length: 146.20\tAvg Reward: -7.834\n",
      "2025-01-26 23:46:03 [INFO] Episode 2320\tAvg Length: 157.55\tAvg Reward: -8.072\n",
      "2025-01-26 23:46:16 [INFO] Episode 2340\tAvg Length: 205.15\tAvg Reward: -8.846\n",
      "2025-01-26 23:46:29 [INFO] Episode 2360\tAvg Length: 170.20\tAvg Reward: -8.275\n",
      "2025-01-26 23:46:42 [INFO] Episode 2380\tAvg Length: 180.60\tAvg Reward: -5.196\n",
      "2025-01-26 23:46:55 [INFO] Episode 2400\tAvg Length: 123.40\tAvg Reward: -3.858\n",
      "2025-01-26 23:47:08 [INFO] Episode 2420\tAvg Length: 137.30\tAvg Reward: -1.600\n",
      "2025-01-26 23:47:21 [INFO] Episode 2440\tAvg Length: 168.20\tAvg Reward: -3.711\n",
      "2025-01-26 23:47:35 [INFO] Episode 2460\tAvg Length: 140.40\tAvg Reward: -8.464\n",
      "2025-01-26 23:47:48 [INFO] Episode 2480\tAvg Length: 145.00\tAvg Reward: -9.272\n",
      "2025-01-26 23:48:01 [INFO] Episode 2500\tAvg Length: 155.30\tAvg Reward: -1.353\n",
      "2025-01-26 23:48:14 [INFO] Episode 2520\tAvg Length: 138.50\tAvg Reward: -2.798\n",
      "2025-01-26 23:48:27 [INFO] Episode 2540\tAvg Length: 165.55\tAvg Reward: -6.613\n",
      "2025-01-26 23:48:40 [INFO] Episode 2560\tAvg Length: 140.95\tAvg Reward: -6.514\n",
      "2025-01-26 23:48:53 [INFO] Episode 2580\tAvg Length: 156.55\tAvg Reward: -7.205\n",
      "2025-01-26 23:49:07 [INFO] Episode 2600\tAvg Length: 169.25\tAvg Reward: -6.713\n",
      "2025-01-26 23:49:20 [INFO] Episode 2620\tAvg Length: 178.80\tAvg Reward: -9.006\n",
      "2025-01-26 23:49:34 [INFO] Episode 2640\tAvg Length: 159.30\tAvg Reward: -5.668\n",
      "2025-01-26 23:49:47 [INFO] Episode 2660\tAvg Length: 144.35\tAvg Reward: -5.038\n",
      "2025-01-26 23:50:01 [INFO] Episode 2680\tAvg Length: 136.80\tAvg Reward: -4.360\n",
      "2025-01-26 23:50:14 [INFO] Episode 2700\tAvg Length: 213.50\tAvg Reward: -7.595\n",
      "2025-01-26 23:50:28 [INFO] Episode 2720\tAvg Length: 150.10\tAvg Reward: -6.432\n",
      "2025-01-26 23:50:42 [INFO] Episode 2740\tAvg Length: 152.20\tAvg Reward: -4.025\n",
      "2025-01-26 23:50:56 [INFO] Episode 2760\tAvg Length: 137.25\tAvg Reward: -2.800\n",
      "2025-01-26 23:51:10 [INFO] Episode 2780\tAvg Length: 167.55\tAvg Reward: -7.213\n",
      "2025-01-26 23:51:24 [INFO] Episode 2800\tAvg Length: 134.65\tAvg Reward: -2.688\n",
      "2025-01-26 23:51:38 [INFO] Episode 2820\tAvg Length: 127.55\tAvg Reward: -3.804\n",
      "2025-01-26 23:51:53 [INFO] Episode 2840\tAvg Length: 167.20\tAvg Reward: -7.501\n",
      "2025-01-26 23:52:07 [INFO] Episode 2860\tAvg Length: 153.45\tAvg Reward: -9.071\n",
      "2025-01-26 23:52:22 [INFO] Episode 2880\tAvg Length: 170.25\tAvg Reward: -5.577\n",
      "2025-01-26 23:52:36 [INFO] Episode 2900\tAvg Length: 151.20\tAvg Reward: -4.135\n",
      "2025-01-26 23:52:51 [INFO] Episode 2920\tAvg Length: 142.75\tAvg Reward: -3.698\n",
      "2025-01-26 23:53:05 [INFO] Episode 2940\tAvg Length: 153.55\tAvg Reward: -2.433\n",
      "2025-01-26 23:53:20 [INFO] Episode 2960\tAvg Length: 129.60\tAvg Reward: -2.728\n",
      "2025-01-26 23:53:35 [INFO] Episode 2980\tAvg Length: 149.50\tAvg Reward: -2.918\n",
      "2025-01-26 23:53:50 [INFO] Saved checkpoint at episode 3000 -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/saved_models/DDPG_HockeyEnv_noise0.1_alr0.0001_clr0.0001_gamma0.95_checkpoint_ep3000.pth\n",
      "2025-01-26 23:53:50 [INFO] Saved training statistics to -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/stats/DDPG_HockeyEnv_noise0.1_alr0.0001_clr0.0001_gamma0.95_stats.pkl\n",
      "2025-01-26 23:53:50 [INFO] Episode 3000\tAvg Length: 146.60\tAvg Reward: -5.096\n",
      "2025-01-26 23:54:05 [INFO] Episode 3020\tAvg Length: 156.65\tAvg Reward: -8.627\n",
      "2025-01-26 23:54:20 [INFO] Episode 3040\tAvg Length: 179.35\tAvg Reward: -6.541\n",
      "2025-01-26 23:54:35 [INFO] Episode 3060\tAvg Length: 130.80\tAvg Reward: -3.965\n",
      "2025-01-26 23:54:51 [INFO] Episode 3080\tAvg Length: 148.45\tAvg Reward: -4.253\n",
      "2025-01-26 23:55:06 [INFO] Episode 3100\tAvg Length: 147.45\tAvg Reward: -7.426\n",
      "2025-01-26 23:55:21 [INFO] Episode 3120\tAvg Length: 178.85\tAvg Reward: -8.096\n",
      "2025-01-26 23:55:37 [INFO] Episode 3140\tAvg Length: 160.40\tAvg Reward: -8.063\n",
      "2025-01-26 23:55:52 [INFO] Episode 3160\tAvg Length: 171.80\tAvg Reward: -6.393\n",
      "2025-01-26 23:56:08 [INFO] Episode 3180\tAvg Length: 137.95\tAvg Reward: -4.643\n",
      "2025-01-26 23:56:24 [INFO] Episode 3200\tAvg Length: 164.85\tAvg Reward: -6.515\n",
      "2025-01-26 23:56:40 [INFO] Episode 3220\tAvg Length: 157.95\tAvg Reward: -6.667\n",
      "2025-01-26 23:56:56 [INFO] Episode 3240\tAvg Length: 163.85\tAvg Reward: -4.969\n",
      "2025-01-26 23:57:12 [INFO] Episode 3260\tAvg Length: 167.90\tAvg Reward: -7.115\n",
      "2025-01-26 23:57:28 [INFO] Episode 3280\tAvg Length: 141.05\tAvg Reward: -1.858\n",
      "2025-01-26 23:57:44 [INFO] Episode 3300\tAvg Length: 181.25\tAvg Reward: -7.504\n",
      "2025-01-26 23:58:00 [INFO] Episode 3320\tAvg Length: 120.10\tAvg Reward: -4.339\n",
      "2025-01-26 23:58:16 [INFO] Episode 3340\tAvg Length: 180.00\tAvg Reward: -2.966\n",
      "2025-01-26 23:58:33 [INFO] Episode 3360\tAvg Length: 189.40\tAvg Reward: -4.982\n",
      "2025-01-26 23:58:49 [INFO] Episode 3380\tAvg Length: 195.85\tAvg Reward: -6.957\n",
      "2025-01-26 23:59:06 [INFO] Episode 3400\tAvg Length: 184.90\tAvg Reward: -10.467\n",
      "2025-01-26 23:59:23 [INFO] Episode 3420\tAvg Length: 168.10\tAvg Reward: -5.903\n",
      "2025-01-26 23:59:40 [INFO] Episode 3440\tAvg Length: 170.00\tAvg Reward: -5.835\n",
      "2025-01-26 23:59:57 [INFO] Episode 3460\tAvg Length: 132.10\tAvg Reward: -0.081\n",
      "2025-01-27 00:00:14 [INFO] Episode 3480\tAvg Length: 145.55\tAvg Reward: -2.011\n",
      "2025-01-27 00:00:31 [INFO] Episode 3500\tAvg Length: 178.70\tAvg Reward: -6.620\n",
      "2025-01-27 00:00:48 [INFO] Episode 3520\tAvg Length: 179.10\tAvg Reward: -4.808\n",
      "2025-01-27 00:01:06 [INFO] Episode 3540\tAvg Length: 154.95\tAvg Reward: -6.928\n",
      "2025-01-27 00:01:23 [INFO] Episode 3560\tAvg Length: 155.20\tAvg Reward: -5.630\n",
      "2025-01-27 00:01:41 [INFO] Episode 3580\tAvg Length: 150.60\tAvg Reward: -4.296\n",
      "2025-01-27 00:01:58 [INFO] Episode 3600\tAvg Length: 133.50\tAvg Reward: -2.019\n",
      "2025-01-27 00:02:16 [INFO] Episode 3620\tAvg Length: 157.55\tAvg Reward: -4.014\n",
      "2025-01-27 00:02:34 [INFO] Episode 3640\tAvg Length: 138.85\tAvg Reward: -2.356\n",
      "2025-01-27 00:02:52 [INFO] Episode 3660\tAvg Length: 126.65\tAvg Reward: -0.603\n",
      "2025-01-27 00:03:09 [INFO] Episode 3680\tAvg Length: 137.25\tAvg Reward: -6.623\n",
      "2025-01-27 00:03:27 [INFO] Episode 3700\tAvg Length: 115.20\tAvg Reward: -6.243\n",
      "2025-01-27 00:03:45 [INFO] Episode 3720\tAvg Length: 135.20\tAvg Reward: -5.073\n",
      "2025-01-27 00:04:04 [INFO] Episode 3740\tAvg Length: 149.95\tAvg Reward: -2.422\n",
      "2025-01-27 00:04:22 [INFO] Episode 3760\tAvg Length: 119.20\tAvg Reward: -3.419\n",
      "2025-01-27 00:04:40 [INFO] Episode 3780\tAvg Length: 155.65\tAvg Reward: -2.327\n",
      "2025-01-27 00:04:59 [INFO] Episode 3800\tAvg Length: 141.20\tAvg Reward: -0.746\n",
      "2025-01-27 00:05:17 [INFO] Episode 3820\tAvg Length: 143.05\tAvg Reward: -7.054\n",
      "2025-01-27 00:05:36 [INFO] Episode 3840\tAvg Length: 139.45\tAvg Reward: -0.928\n",
      "2025-01-27 00:05:55 [INFO] Episode 3860\tAvg Length: 160.25\tAvg Reward: -7.093\n",
      "2025-01-27 00:06:14 [INFO] Episode 3880\tAvg Length: 112.80\tAvg Reward: -1.455\n",
      "2025-01-27 00:06:32 [INFO] Episode 3900\tAvg Length: 129.70\tAvg Reward: -0.950\n",
      "2025-01-27 00:06:51 [INFO] Episode 3920\tAvg Length: 190.05\tAvg Reward: -6.670\n",
      "2025-01-27 00:07:11 [INFO] Episode 3940\tAvg Length: 142.35\tAvg Reward: -2.404\n",
      "2025-01-27 00:07:30 [INFO] Episode 3960\tAvg Length: 145.55\tAvg Reward: -3.997\n",
      "2025-01-27 00:07:49 [INFO] Episode 3980\tAvg Length: 81.40\tAvg Reward: -1.284\n",
      "2025-01-27 00:08:08 [INFO] Saved checkpoint at episode 4000 -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/saved_models/DDPG_HockeyEnv_noise0.1_alr0.0001_clr0.0001_gamma0.95_checkpoint_ep4000.pth\n",
      "2025-01-27 00:08:08 [INFO] Saved training statistics to -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/stats/DDPG_HockeyEnv_noise0.1_alr0.0001_clr0.0001_gamma0.95_stats.pkl\n",
      "2025-01-27 00:08:08 [INFO] Episode 4000\tAvg Length: 144.05\tAvg Reward: -2.593\n",
      "2025-01-27 00:08:28 [INFO] Episode 4020\tAvg Length: 149.45\tAvg Reward: -3.551\n",
      "2025-01-27 00:08:47 [INFO] Episode 4040\tAvg Length: 135.10\tAvg Reward: -2.385\n",
      "2025-01-27 00:09:07 [INFO] Episode 4060\tAvg Length: 175.70\tAvg Reward: -6.042\n",
      "2025-01-27 00:09:26 [INFO] Episode 4080\tAvg Length: 151.55\tAvg Reward: -2.218\n",
      "2025-01-27 00:09:46 [INFO] Episode 4100\tAvg Length: 144.55\tAvg Reward: -2.605\n",
      "2025-01-27 00:10:06 [INFO] Episode 4120\tAvg Length: 156.85\tAvg Reward: -3.868\n",
      "2025-01-27 00:10:26 [INFO] Episode 4140\tAvg Length: 171.70\tAvg Reward: -9.730\n",
      "2025-01-27 00:10:46 [INFO] Episode 4160\tAvg Length: 184.95\tAvg Reward: -3.585\n",
      "2025-01-27 00:11:06 [INFO] Episode 4180\tAvg Length: 145.80\tAvg Reward: -2.864\n",
      "2025-01-27 00:11:27 [INFO] Episode 4200\tAvg Length: 146.35\tAvg Reward: -4.464\n",
      "2025-01-27 00:11:47 [INFO] Episode 4220\tAvg Length: 113.40\tAvg Reward: -3.163\n",
      "2025-01-27 00:12:08 [INFO] Episode 4240\tAvg Length: 195.90\tAvg Reward: -5.982\n",
      "2025-01-27 00:12:28 [INFO] Episode 4260\tAvg Length: 145.55\tAvg Reward: -3.554\n",
      "2025-01-27 00:12:50 [INFO] Episode 4280\tAvg Length: 141.50\tAvg Reward: -3.607\n",
      "2025-01-27 00:13:13 [INFO] Episode 4300\tAvg Length: 145.75\tAvg Reward: -3.658\n",
      "2025-01-27 00:13:34 [INFO] Episode 4320\tAvg Length: 145.25\tAvg Reward: -6.483\n",
      "2025-01-27 00:13:55 [INFO] Episode 4340\tAvg Length: 175.75\tAvg Reward: -3.749\n",
      "2025-01-27 00:14:17 [INFO] Episode 4360\tAvg Length: 159.40\tAvg Reward: -2.189\n",
      "2025-01-27 00:14:38 [INFO] Episode 4380\tAvg Length: 183.45\tAvg Reward: -8.027\n",
      "2025-01-27 00:15:00 [INFO] Episode 4400\tAvg Length: 115.90\tAvg Reward: -0.970\n",
      "2025-01-27 00:15:21 [INFO] Episode 4420\tAvg Length: 118.40\tAvg Reward: -6.220\n",
      "2025-01-27 00:15:42 [INFO] Episode 4440\tAvg Length: 133.85\tAvg Reward: -6.750\n",
      "2025-01-27 00:16:04 [INFO] Episode 4460\tAvg Length: 161.60\tAvg Reward: -4.686\n",
      "2025-01-27 00:16:25 [INFO] Episode 4480\tAvg Length: 108.80\tAvg Reward: -5.697\n",
      "2025-01-27 00:16:47 [INFO] Episode 4500\tAvg Length: 136.95\tAvg Reward: -1.547\n",
      "2025-01-27 00:17:09 [INFO] Episode 4520\tAvg Length: 144.65\tAvg Reward: -2.749\n",
      "2025-01-27 00:17:31 [INFO] Episode 4540\tAvg Length: 147.15\tAvg Reward: -4.605\n",
      "2025-01-27 00:17:52 [INFO] Episode 4560\tAvg Length: 90.35\tAvg Reward: 0.595\n",
      "2025-01-27 00:18:14 [INFO] Episode 4580\tAvg Length: 137.95\tAvg Reward: -3.620\n",
      "2025-01-27 00:18:36 [INFO] Episode 4600\tAvg Length: 118.00\tAvg Reward: -5.712\n",
      "2025-01-27 00:18:59 [INFO] Episode 4620\tAvg Length: 192.65\tAvg Reward: -5.490\n",
      "2025-01-27 00:19:21 [INFO] Episode 4640\tAvg Length: 132.10\tAvg Reward: -1.858\n",
      "2025-01-27 00:19:44 [INFO] Episode 4660\tAvg Length: 144.40\tAvg Reward: -1.842\n",
      "2025-01-27 00:20:06 [INFO] Episode 4680\tAvg Length: 146.95\tAvg Reward: -1.868\n",
      "2025-01-27 00:20:29 [INFO] Episode 4700\tAvg Length: 152.40\tAvg Reward: -2.510\n",
      "2025-01-27 00:20:51 [INFO] Episode 4720\tAvg Length: 153.75\tAvg Reward: -4.248\n",
      "2025-01-27 00:21:14 [INFO] Episode 4740\tAvg Length: 148.20\tAvg Reward: -1.667\n",
      "2025-01-27 00:21:37 [INFO] Episode 4760\tAvg Length: 188.80\tAvg Reward: -8.775\n",
      "2025-01-27 00:22:00 [INFO] Episode 4780\tAvg Length: 141.40\tAvg Reward: -1.803\n",
      "2025-01-27 00:22:23 [INFO] Episode 4800\tAvg Length: 170.45\tAvg Reward: -5.015\n",
      "2025-01-27 00:22:47 [INFO] Episode 4820\tAvg Length: 136.95\tAvg Reward: -4.186\n",
      "2025-01-27 00:23:10 [INFO] Episode 4840\tAvg Length: 165.40\tAvg Reward: -6.200\n",
      "2025-01-27 00:23:33 [INFO] Episode 4860\tAvg Length: 156.90\tAvg Reward: -5.682\n",
      "2025-01-27 00:23:57 [INFO] Episode 4880\tAvg Length: 129.80\tAvg Reward: -1.185\n",
      "2025-01-27 00:24:21 [INFO] Episode 4900\tAvg Length: 136.65\tAvg Reward: -2.071\n",
      "2025-01-27 00:24:44 [INFO] Episode 4920\tAvg Length: 143.00\tAvg Reward: -0.818\n",
      "2025-01-27 00:25:08 [INFO] Episode 4940\tAvg Length: 144.30\tAvg Reward: -5.626\n",
      "2025-01-27 00:25:32 [INFO] Episode 4960\tAvg Length: 155.05\tAvg Reward: -7.215\n",
      "2025-01-27 00:25:55 [INFO] Episode 4980\tAvg Length: 151.25\tAvg Reward: -6.447\n",
      "2025-01-27 00:26:19 [INFO] Saved checkpoint at episode 5000 -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/saved_models/DDPG_HockeyEnv_noise0.1_alr0.0001_clr0.0001_gamma0.95_checkpoint_ep5000.pth\n",
      "2025-01-27 00:26:19 [INFO] Saved training statistics to -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/stats/DDPG_HockeyEnv_noise0.1_alr0.0001_clr0.0001_gamma0.95_stats.pkl\n",
      "2025-01-27 00:26:19 [INFO] Episode 5000\tAvg Length: 157.25\tAvg Reward: -2.302\n",
      "2025-01-27 00:26:44 [INFO] Episode 5020\tAvg Length: 158.80\tAvg Reward: -3.122\n",
      "2025-01-27 00:27:08 [INFO] Episode 5040\tAvg Length: 138.70\tAvg Reward: -3.690\n",
      "2025-01-27 00:27:32 [INFO] Episode 5060\tAvg Length: 144.75\tAvg Reward: -2.332\n",
      "2025-01-27 00:27:56 [INFO] Episode 5080\tAvg Length: 152.50\tAvg Reward: -4.723\n",
      "2025-01-27 00:28:21 [INFO] Episode 5100\tAvg Length: 128.45\tAvg Reward: -2.303\n",
      "2025-01-27 00:28:45 [INFO] Episode 5120\tAvg Length: 138.40\tAvg Reward: -4.175\n",
      "2025-01-27 00:29:10 [INFO] Episode 5140\tAvg Length: 177.60\tAvg Reward: -6.028\n",
      "2025-01-27 00:29:35 [INFO] Episode 5160\tAvg Length: 132.15\tAvg Reward: -2.783\n",
      "2025-01-27 00:29:59 [INFO] Episode 5180\tAvg Length: 160.90\tAvg Reward: -4.676\n",
      "2025-01-27 00:30:24 [INFO] Episode 5200\tAvg Length: 177.45\tAvg Reward: -8.685\n",
      "2025-01-27 00:30:49 [INFO] Episode 5220\tAvg Length: 148.05\tAvg Reward: -3.166\n",
      "2025-01-27 00:31:14 [INFO] Episode 5240\tAvg Length: 133.55\tAvg Reward: -5.453\n",
      "2025-01-27 00:31:39 [INFO] Episode 5260\tAvg Length: 126.05\tAvg Reward: -0.952\n",
      "2025-01-27 00:32:04 [INFO] Episode 5280\tAvg Length: 127.60\tAvg Reward: -1.662\n",
      "2025-01-27 00:32:29 [INFO] Episode 5300\tAvg Length: 100.35\tAvg Reward: -1.253\n",
      "2025-01-27 00:32:55 [INFO] Episode 5320\tAvg Length: 147.80\tAvg Reward: -1.799\n",
      "2025-01-27 00:33:20 [INFO] Episode 5340\tAvg Length: 189.40\tAvg Reward: -4.147\n",
      "2025-01-27 00:33:46 [INFO] Episode 5360\tAvg Length: 129.30\tAvg Reward: -0.502\n",
      "2025-01-27 00:34:11 [INFO] Episode 5380\tAvg Length: 151.40\tAvg Reward: 0.499\n",
      "2025-01-27 00:34:37 [INFO] Episode 5400\tAvg Length: 141.10\tAvg Reward: -1.634\n",
      "2025-01-27 00:35:03 [INFO] Episode 5420\tAvg Length: 128.10\tAvg Reward: 0.115\n",
      "2025-01-27 00:35:29 [INFO] Episode 5440\tAvg Length: 187.10\tAvg Reward: -4.806\n",
      "2025-01-27 00:35:55 [INFO] Episode 5460\tAvg Length: 168.65\tAvg Reward: -5.926\n",
      "2025-01-27 00:36:21 [INFO] Episode 5480\tAvg Length: 173.05\tAvg Reward: -3.826\n",
      "2025-01-27 00:36:47 [INFO] Episode 5500\tAvg Length: 117.10\tAvg Reward: -1.051\n",
      "2025-01-27 00:37:13 [INFO] Episode 5520\tAvg Length: 158.30\tAvg Reward: -9.172\n",
      "2025-01-27 00:37:39 [INFO] Episode 5540\tAvg Length: 153.40\tAvg Reward: -6.197\n",
      "2025-01-27 00:38:05 [INFO] Episode 5560\tAvg Length: 156.15\tAvg Reward: -6.034\n",
      "2025-01-27 00:38:32 [INFO] Episode 5580\tAvg Length: 132.85\tAvg Reward: -0.097\n",
      "2025-01-27 00:38:58 [INFO] Episode 5600\tAvg Length: 157.45\tAvg Reward: -0.665\n",
      "2025-01-27 00:39:25 [INFO] Episode 5620\tAvg Length: 143.85\tAvg Reward: 0.328\n",
      "2025-01-27 00:39:52 [INFO] Episode 5640\tAvg Length: 116.40\tAvg Reward: -2.833\n",
      "2025-01-27 00:40:19 [INFO] Episode 5660\tAvg Length: 150.80\tAvg Reward: -4.829\n",
      "2025-01-27 00:40:45 [INFO] Episode 5680\tAvg Length: 137.95\tAvg Reward: -2.613\n",
      "2025-01-27 00:41:12 [INFO] Episode 5700\tAvg Length: 140.00\tAvg Reward: -1.373\n",
      "2025-01-27 00:41:39 [INFO] Episode 5720\tAvg Length: 151.30\tAvg Reward: -2.700\n",
      "2025-01-27 00:42:06 [INFO] Episode 5740\tAvg Length: 137.20\tAvg Reward: -5.651\n",
      "2025-01-27 00:42:33 [INFO] Episode 5760\tAvg Length: 153.95\tAvg Reward: -3.058\n",
      "2025-01-27 00:43:00 [INFO] Episode 5780\tAvg Length: 159.65\tAvg Reward: -3.301\n",
      "2025-01-27 00:43:28 [INFO] Episode 5800\tAvg Length: 143.15\tAvg Reward: -2.824\n",
      "2025-01-27 00:43:55 [INFO] Episode 5820\tAvg Length: 161.25\tAvg Reward: -5.369\n",
      "2025-01-27 00:44:23 [INFO] Episode 5840\tAvg Length: 162.30\tAvg Reward: -2.531\n",
      "2025-01-27 00:44:50 [INFO] Episode 5860\tAvg Length: 134.90\tAvg Reward: -3.508\n",
      "2025-01-27 00:45:17 [INFO] Episode 5880\tAvg Length: 180.95\tAvg Reward: -5.459\n",
      "2025-01-27 00:45:45 [INFO] Episode 5900\tAvg Length: 143.35\tAvg Reward: -3.614\n",
      "2025-01-27 00:46:12 [INFO] Episode 5920\tAvg Length: 114.70\tAvg Reward: 0.127\n",
      "2025-01-27 00:46:40 [INFO] Episode 5940\tAvg Length: 182.90\tAvg Reward: -4.048\n",
      "2025-01-27 00:47:07 [INFO] Episode 5960\tAvg Length: 123.15\tAvg Reward: -0.904\n",
      "2025-01-27 00:47:35 [INFO] Episode 5980\tAvg Length: 180.80\tAvg Reward: -4.633\n",
      "2025-01-27 00:48:03 [INFO] Saved checkpoint at episode 6000 -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/saved_models/DDPG_HockeyEnv_noise0.1_alr0.0001_clr0.0001_gamma0.95_checkpoint_ep6000.pth\n",
      "2025-01-27 00:48:03 [INFO] Saved training statistics to -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/stats/DDPG_HockeyEnv_noise0.1_alr0.0001_clr0.0001_gamma0.95_stats.pkl\n",
      "2025-01-27 00:48:03 [INFO] Episode 6000\tAvg Length: 155.50\tAvg Reward: -2.303\n",
      "2025-01-27 00:48:31 [INFO] Episode 6020\tAvg Length: 192.75\tAvg Reward: -10.338\n",
      "2025-01-27 00:48:59 [INFO] Episode 6040\tAvg Length: 135.65\tAvg Reward: -1.379\n",
      "2025-01-27 00:49:27 [INFO] Episode 6060\tAvg Length: 143.30\tAvg Reward: -2.674\n",
      "2025-01-27 00:49:56 [INFO] Episode 6080\tAvg Length: 163.90\tAvg Reward: -1.866\n",
      "2025-01-27 00:50:24 [INFO] Episode 6100\tAvg Length: 157.00\tAvg Reward: -1.960\n",
      "2025-01-27 00:50:53 [INFO] Episode 6120\tAvg Length: 149.30\tAvg Reward: -0.220\n",
      "2025-01-27 00:51:21 [INFO] Episode 6140\tAvg Length: 142.10\tAvg Reward: -3.845\n",
      "2025-01-27 00:51:50 [INFO] Episode 6160\tAvg Length: 136.50\tAvg Reward: -0.124\n",
      "2025-01-27 00:52:18 [INFO] Episode 6180\tAvg Length: 163.65\tAvg Reward: -4.095\n",
      "2025-01-27 00:52:47 [INFO] Episode 6200\tAvg Length: 172.00\tAvg Reward: -6.219\n",
      "2025-01-27 00:53:16 [INFO] Episode 6220\tAvg Length: 117.00\tAvg Reward: -3.594\n",
      "2025-01-27 00:53:45 [INFO] Episode 6240\tAvg Length: 130.80\tAvg Reward: 0.452\n",
      "2025-01-27 00:54:14 [INFO] Episode 6260\tAvg Length: 142.45\tAvg Reward: -3.335\n",
      "2025-01-27 00:54:43 [INFO] Episode 6280\tAvg Length: 171.85\tAvg Reward: -4.007\n",
      "2025-01-27 00:55:12 [INFO] Episode 6300\tAvg Length: 132.60\tAvg Reward: -5.326\n",
      "2025-01-27 00:55:41 [INFO] Episode 6320\tAvg Length: 130.05\tAvg Reward: -5.660\n",
      "2025-01-27 00:56:10 [INFO] Episode 6340\tAvg Length: 112.45\tAvg Reward: -5.136\n",
      "2025-01-27 00:56:40 [INFO] Episode 6360\tAvg Length: 154.85\tAvg Reward: -3.845\n",
      "2025-01-27 00:57:09 [INFO] Episode 6380\tAvg Length: 119.50\tAvg Reward: -0.184\n",
      "2025-01-27 00:57:38 [INFO] Episode 6400\tAvg Length: 103.60\tAvg Reward: -0.346\n",
      "2025-01-27 00:58:08 [INFO] Episode 6420\tAvg Length: 160.35\tAvg Reward: -3.732\n",
      "2025-01-27 00:58:38 [INFO] Episode 6440\tAvg Length: 173.35\tAvg Reward: -6.027\n",
      "2025-01-27 00:59:07 [INFO] Episode 6460\tAvg Length: 141.60\tAvg Reward: -0.921\n",
      "2025-01-27 00:59:37 [INFO] Episode 6480\tAvg Length: 167.70\tAvg Reward: -4.721\n",
      "2025-01-27 01:00:07 [INFO] Episode 6500\tAvg Length: 181.45\tAvg Reward: -8.774\n",
      "2025-01-27 01:00:37 [INFO] Episode 6520\tAvg Length: 135.05\tAvg Reward: -0.946\n",
      "2025-01-27 01:01:07 [INFO] Episode 6540\tAvg Length: 115.95\tAvg Reward: -5.648\n",
      "2025-01-27 01:01:37 [INFO] Episode 6560\tAvg Length: 147.60\tAvg Reward: -1.666\n",
      "2025-01-27 01:02:07 [INFO] Episode 6580\tAvg Length: 136.40\tAvg Reward: -4.001\n",
      "2025-01-27 01:02:37 [INFO] Episode 6600\tAvg Length: 149.50\tAvg Reward: -3.052\n",
      "2025-01-27 01:03:07 [INFO] Episode 6620\tAvg Length: 153.30\tAvg Reward: -4.916\n",
      "2025-01-27 01:03:37 [INFO] Episode 6640\tAvg Length: 152.95\tAvg Reward: -3.488\n",
      "2025-01-27 01:04:08 [INFO] Episode 6660\tAvg Length: 121.30\tAvg Reward: -3.841\n",
      "2025-01-27 01:04:38 [INFO] Episode 6680\tAvg Length: 160.65\tAvg Reward: -2.449\n",
      "2025-01-27 01:05:08 [INFO] Episode 6700\tAvg Length: 162.40\tAvg Reward: -1.471\n",
      "2025-01-27 01:05:38 [INFO] Episode 6720\tAvg Length: 144.55\tAvg Reward: -1.070\n",
      "2025-01-27 01:06:09 [INFO] Episode 6740\tAvg Length: 162.70\tAvg Reward: -4.415\n",
      "2025-01-27 01:06:39 [INFO] Episode 6760\tAvg Length: 157.70\tAvg Reward: -7.668\n",
      "2025-01-27 01:07:09 [INFO] Episode 6780\tAvg Length: 167.35\tAvg Reward: -4.474\n",
      "2025-01-27 01:07:39 [INFO] Episode 6800\tAvg Length: 162.85\tAvg Reward: -4.003\n",
      "2025-01-27 01:08:09 [INFO] Episode 6820\tAvg Length: 80.00\tAvg Reward: -2.493\n",
      "2025-01-27 01:08:39 [INFO] Episode 6840\tAvg Length: 119.25\tAvg Reward: -1.894\n",
      "2025-01-27 01:09:10 [INFO] Episode 6860\tAvg Length: 99.65\tAvg Reward: -2.397\n",
      "2025-01-27 01:09:40 [INFO] Episode 6880\tAvg Length: 144.20\tAvg Reward: -2.642\n",
      "2025-01-27 01:10:10 [INFO] Episode 6900\tAvg Length: 165.80\tAvg Reward: -4.810\n",
      "2025-01-27 01:10:41 [INFO] Episode 6920\tAvg Length: 181.15\tAvg Reward: -5.152\n",
      "2025-01-27 01:11:12 [INFO] Episode 6940\tAvg Length: 172.90\tAvg Reward: -2.591\n",
      "2025-01-27 01:11:42 [INFO] Episode 6960\tAvg Length: 121.35\tAvg Reward: -4.988\n",
      "2025-01-27 01:12:13 [INFO] Episode 6980\tAvg Length: 152.00\tAvg Reward: -4.432\n",
      "2025-01-27 01:12:43 [INFO] Saved checkpoint at episode 7000 -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/saved_models/DDPG_HockeyEnv_noise0.1_alr0.0001_clr0.0001_gamma0.95_checkpoint_ep7000.pth\n",
      "2025-01-27 01:12:43 [INFO] Saved training statistics to -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/stats/DDPG_HockeyEnv_noise0.1_alr0.0001_clr0.0001_gamma0.95_stats.pkl\n",
      "2025-01-27 01:12:43 [INFO] Episode 7000\tAvg Length: 154.20\tAvg Reward: -5.476\n",
      "2025-01-27 01:13:13 [INFO] Episode 7020\tAvg Length: 149.05\tAvg Reward: -7.026\n",
      "2025-01-27 01:13:44 [INFO] Episode 7040\tAvg Length: 127.55\tAvg Reward: -6.528\n",
      "2025-01-27 01:14:14 [INFO] Episode 7060\tAvg Length: 141.40\tAvg Reward: -3.117\n",
      "2025-01-27 01:14:44 [INFO] Episode 7080\tAvg Length: 135.15\tAvg Reward: -4.846\n",
      "2025-01-27 01:15:15 [INFO] Episode 7100\tAvg Length: 159.10\tAvg Reward: -8.832\n",
      "2025-01-27 01:15:45 [INFO] Episode 7120\tAvg Length: 133.40\tAvg Reward: -1.890\n",
      "2025-01-27 01:16:15 [INFO] Episode 7140\tAvg Length: 129.30\tAvg Reward: -4.825\n",
      "2025-01-27 01:16:45 [INFO] Episode 7160\tAvg Length: 141.55\tAvg Reward: -1.174\n",
      "2025-01-27 01:17:15 [INFO] Episode 7180\tAvg Length: 136.55\tAvg Reward: -7.166\n",
      "2025-01-27 01:17:45 [INFO] Episode 7200\tAvg Length: 131.05\tAvg Reward: -4.401\n",
      "2025-01-27 01:18:16 [INFO] Episode 7220\tAvg Length: 150.15\tAvg Reward: -5.175\n",
      "2025-01-27 01:18:46 [INFO] Episode 7240\tAvg Length: 160.90\tAvg Reward: -4.426\n",
      "2025-01-27 01:19:17 [INFO] Episode 7260\tAvg Length: 197.75\tAvg Reward: -5.378\n",
      "2025-01-27 01:19:47 [INFO] Episode 7280\tAvg Length: 150.10\tAvg Reward: -6.057\n",
      "2025-01-27 01:20:17 [INFO] Episode 7300\tAvg Length: 97.05\tAvg Reward: -0.216\n",
      "2025-01-27 01:20:48 [INFO] Episode 7320\tAvg Length: 196.95\tAvg Reward: -3.434\n",
      "2025-01-27 01:21:18 [INFO] Episode 7340\tAvg Length: 145.00\tAvg Reward: -3.940\n",
      "2025-01-27 01:21:48 [INFO] Episode 7360\tAvg Length: 168.05\tAvg Reward: -2.144\n",
      "2025-01-27 01:22:18 [INFO] Episode 7380\tAvg Length: 183.40\tAvg Reward: -7.326\n",
      "2025-01-27 01:22:49 [INFO] Episode 7400\tAvg Length: 161.10\tAvg Reward: -6.017\n",
      "2025-01-27 01:23:19 [INFO] Episode 7420\tAvg Length: 158.15\tAvg Reward: -4.135\n",
      "2025-01-27 01:23:49 [INFO] Episode 7440\tAvg Length: 154.60\tAvg Reward: -7.699\n",
      "2025-01-27 01:24:20 [INFO] Episode 7460\tAvg Length: 119.10\tAvg Reward: -0.970\n",
      "2025-01-27 01:24:50 [INFO] Episode 7480\tAvg Length: 160.30\tAvg Reward: -3.390\n",
      "2025-01-27 01:25:20 [INFO] Episode 7500\tAvg Length: 136.55\tAvg Reward: -5.065\n",
      "2025-01-27 01:25:51 [INFO] Episode 7520\tAvg Length: 151.45\tAvg Reward: -4.828\n",
      "2025-01-27 01:26:21 [INFO] Episode 7540\tAvg Length: 151.00\tAvg Reward: -7.095\n",
      "2025-01-27 01:26:51 [INFO] Episode 7560\tAvg Length: 171.60\tAvg Reward: -2.680\n",
      "2025-01-27 01:27:22 [INFO] Episode 7580\tAvg Length: 160.45\tAvg Reward: -1.794\n",
      "2025-01-27 01:27:52 [INFO] Episode 7600\tAvg Length: 125.60\tAvg Reward: -0.666\n",
      "2025-01-27 01:28:22 [INFO] Episode 7620\tAvg Length: 122.75\tAvg Reward: -3.019\n",
      "2025-01-27 01:28:53 [INFO] Episode 7640\tAvg Length: 133.90\tAvg Reward: 0.912\n",
      "2025-01-27 01:29:23 [INFO] Episode 7660\tAvg Length: 160.25\tAvg Reward: -4.550\n",
      "2025-01-27 01:29:53 [INFO] Episode 7680\tAvg Length: 151.10\tAvg Reward: -3.713\n",
      "2025-01-27 01:30:24 [INFO] Episode 7700\tAvg Length: 114.40\tAvg Reward: -0.239\n",
      "2025-01-27 01:30:54 [INFO] Episode 7720\tAvg Length: 142.00\tAvg Reward: -2.629\n",
      "2025-01-27 01:31:24 [INFO] Episode 7740\tAvg Length: 157.70\tAvg Reward: -2.165\n",
      "2025-01-27 01:31:54 [INFO] Episode 7760\tAvg Length: 132.70\tAvg Reward: -5.218\n",
      "2025-01-27 01:32:25 [INFO] Episode 7780\tAvg Length: 146.30\tAvg Reward: -0.767\n",
      "2025-01-27 01:32:55 [INFO] Episode 7800\tAvg Length: 157.35\tAvg Reward: -4.736\n",
      "2025-01-27 01:33:25 [INFO] Episode 7820\tAvg Length: 129.35\tAvg Reward: -6.040\n",
      "2025-01-27 01:33:55 [INFO] Episode 7840\tAvg Length: 98.80\tAvg Reward: -0.609\n",
      "2025-01-27 01:34:26 [INFO] Episode 7860\tAvg Length: 139.40\tAvg Reward: -2.070\n",
      "2025-01-27 01:34:56 [INFO] Episode 7880\tAvg Length: 157.30\tAvg Reward: -1.134\n",
      "2025-01-27 01:35:26 [INFO] Episode 7900\tAvg Length: 153.85\tAvg Reward: -6.073\n",
      "2025-01-27 01:35:56 [INFO] Episode 7920\tAvg Length: 131.75\tAvg Reward: -6.498\n",
      "2025-01-27 01:36:26 [INFO] Episode 7940\tAvg Length: 130.75\tAvg Reward: -4.561\n",
      "2025-01-27 01:36:57 [INFO] Episode 7960\tAvg Length: 120.20\tAvg Reward: -0.130\n",
      "2025-01-27 01:37:27 [INFO] Episode 7980\tAvg Length: 139.05\tAvg Reward: -4.834\n",
      "2025-01-27 01:37:57 [INFO] Saved checkpoint at episode 8000 -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/saved_models/DDPG_HockeyEnv_noise0.1_alr0.0001_clr0.0001_gamma0.95_checkpoint_ep8000.pth\n",
      "2025-01-27 01:37:57 [INFO] Saved training statistics to -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/stats/DDPG_HockeyEnv_noise0.1_alr0.0001_clr0.0001_gamma0.95_stats.pkl\n",
      "2025-01-27 01:37:57 [INFO] Episode 8000\tAvg Length: 166.90\tAvg Reward: -0.925\n",
      "2025-01-27 01:38:28 [INFO] Episode 8020\tAvg Length: 133.90\tAvg Reward: -0.425\n",
      "2025-01-27 01:38:58 [INFO] Episode 8040\tAvg Length: 126.45\tAvg Reward: -2.611\n",
      "2025-01-27 01:39:28 [INFO] Episode 8060\tAvg Length: 157.15\tAvg Reward: -4.202\n",
      "2025-01-27 01:39:58 [INFO] Episode 8080\tAvg Length: 148.70\tAvg Reward: -4.029\n",
      "2025-01-27 01:40:29 [INFO] Episode 8100\tAvg Length: 158.60\tAvg Reward: -6.736\n",
      "2025-01-27 01:40:59 [INFO] Episode 8120\tAvg Length: 141.40\tAvg Reward: -1.884\n",
      "2025-01-27 01:41:30 [INFO] Episode 8140\tAvg Length: 148.05\tAvg Reward: -4.680\n",
      "2025-01-27 01:42:00 [INFO] Episode 8160\tAvg Length: 136.05\tAvg Reward: -4.317\n",
      "2025-01-27 01:42:30 [INFO] Episode 8180\tAvg Length: 119.50\tAvg Reward: -1.627\n",
      "2025-01-27 01:43:01 [INFO] Episode 8200\tAvg Length: 151.25\tAvg Reward: -5.099\n",
      "2025-01-27 01:43:31 [INFO] Episode 8220\tAvg Length: 156.45\tAvg Reward: -4.702\n",
      "2025-01-27 01:44:01 [INFO] Episode 8240\tAvg Length: 184.15\tAvg Reward: -6.068\n",
      "2025-01-27 01:44:31 [INFO] Episode 8260\tAvg Length: 101.90\tAvg Reward: -2.360\n",
      "2025-01-27 01:45:01 [INFO] Episode 8280\tAvg Length: 128.75\tAvg Reward: -0.845\n",
      "2025-01-27 01:45:31 [INFO] Episode 8300\tAvg Length: 153.15\tAvg Reward: -4.453\n",
      "2025-01-27 01:46:01 [INFO] Episode 8320\tAvg Length: 137.25\tAvg Reward: -3.261\n",
      "2025-01-27 01:46:32 [INFO] Episode 8340\tAvg Length: 149.85\tAvg Reward: -4.140\n",
      "2025-01-27 01:47:02 [INFO] Episode 8360\tAvg Length: 195.15\tAvg Reward: -7.625\n",
      "2025-01-27 01:47:32 [INFO] Episode 8380\tAvg Length: 116.60\tAvg Reward: -3.381\n",
      "2025-01-27 01:48:02 [INFO] Episode 8400\tAvg Length: 114.70\tAvg Reward: -0.361\n",
      "2025-01-27 01:48:33 [INFO] Episode 8420\tAvg Length: 139.20\tAvg Reward: -0.769\n",
      "2025-01-27 01:49:03 [INFO] Episode 8440\tAvg Length: 139.20\tAvg Reward: -5.303\n",
      "2025-01-27 01:49:33 [INFO] Episode 8460\tAvg Length: 87.50\tAvg Reward: 3.731\n",
      "2025-01-27 01:50:03 [INFO] Episode 8480\tAvg Length: 138.45\tAvg Reward: -2.239\n",
      "2025-01-27 01:50:33 [INFO] Episode 8500\tAvg Length: 115.00\tAvg Reward: 0.250\n",
      "2025-01-27 01:51:03 [INFO] Episode 8520\tAvg Length: 96.80\tAvg Reward: 1.975\n",
      "2025-01-27 01:51:33 [INFO] Episode 8540\tAvg Length: 113.15\tAvg Reward: -2.338\n",
      "2025-01-27 01:52:03 [INFO] Episode 8560\tAvg Length: 111.30\tAvg Reward: -1.986\n",
      "2025-01-27 01:52:34 [INFO] Episode 8580\tAvg Length: 116.30\tAvg Reward: 2.038\n",
      "2025-01-27 01:53:04 [INFO] Episode 8600\tAvg Length: 117.55\tAvg Reward: -2.540\n",
      "2025-01-27 01:53:34 [INFO] Episode 8620\tAvg Length: 101.55\tAvg Reward: -0.042\n",
      "2025-01-27 01:54:04 [INFO] Episode 8640\tAvg Length: 181.00\tAvg Reward: -3.355\n",
      "2025-01-27 01:54:35 [INFO] Episode 8660\tAvg Length: 140.10\tAvg Reward: -3.563\n",
      "2025-01-27 01:55:05 [INFO] Episode 8680\tAvg Length: 116.85\tAvg Reward: -2.280\n",
      "2025-01-27 01:55:35 [INFO] Episode 8700\tAvg Length: 136.60\tAvg Reward: -2.358\n",
      "2025-01-27 01:56:05 [INFO] Episode 8720\tAvg Length: 158.50\tAvg Reward: -3.758\n",
      "2025-01-27 01:56:35 [INFO] Episode 8740\tAvg Length: 147.60\tAvg Reward: -2.959\n",
      "2025-01-27 01:57:05 [INFO] Episode 8760\tAvg Length: 138.75\tAvg Reward: -0.656\n",
      "2025-01-27 01:57:36 [INFO] Episode 8780\tAvg Length: 162.55\tAvg Reward: -4.606\n",
      "2025-01-27 01:58:06 [INFO] Episode 8800\tAvg Length: 153.00\tAvg Reward: -3.312\n",
      "2025-01-27 01:58:36 [INFO] Episode 8820\tAvg Length: 149.35\tAvg Reward: -3.724\n",
      "2025-01-27 01:59:06 [INFO] Episode 8840\tAvg Length: 133.20\tAvg Reward: -1.039\n",
      "2025-01-27 01:59:37 [INFO] Episode 8860\tAvg Length: 97.00\tAvg Reward: 4.253\n",
      "2025-01-27 02:00:07 [INFO] Episode 8880\tAvg Length: 123.65\tAvg Reward: -0.784\n",
      "2025-01-27 02:00:37 [INFO] Episode 8900\tAvg Length: 95.35\tAvg Reward: -1.890\n",
      "2025-01-27 02:01:07 [INFO] Episode 8920\tAvg Length: 121.25\tAvg Reward: 0.082\n",
      "2025-01-27 02:01:37 [INFO] Episode 8940\tAvg Length: 121.80\tAvg Reward: -1.132\n",
      "2025-01-27 02:02:07 [INFO] Episode 8960\tAvg Length: 109.05\tAvg Reward: 0.738\n",
      "2025-01-27 02:02:37 [INFO] Episode 8980\tAvg Length: 126.15\tAvg Reward: -2.278\n",
      "2025-01-27 02:03:07 [INFO] Saved checkpoint at episode 9000 -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/saved_models/DDPG_HockeyEnv_noise0.1_alr0.0001_clr0.0001_gamma0.95_checkpoint_ep9000.pth\n",
      "2025-01-27 02:03:07 [INFO] Saved training statistics to -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/stats/DDPG_HockeyEnv_noise0.1_alr0.0001_clr0.0001_gamma0.95_stats.pkl\n",
      "2025-01-27 02:03:07 [INFO] Episode 9000\tAvg Length: 138.85\tAvg Reward: -0.875\n",
      "2025-01-27 02:03:38 [INFO] Episode 9020\tAvg Length: 128.05\tAvg Reward: 0.990\n",
      "2025-01-27 02:04:08 [INFO] Episode 9040\tAvg Length: 122.00\tAvg Reward: 1.659\n",
      "2025-01-27 02:04:38 [INFO] Episode 9060\tAvg Length: 148.55\tAvg Reward: -2.202\n",
      "2025-01-27 02:05:08 [INFO] Episode 9080\tAvg Length: 137.60\tAvg Reward: -2.493\n",
      "2025-01-27 02:05:38 [INFO] Episode 9100\tAvg Length: 135.90\tAvg Reward: -2.658\n",
      "2025-01-27 02:06:08 [INFO] Episode 9120\tAvg Length: 87.45\tAvg Reward: -0.450\n",
      "2025-01-27 02:06:38 [INFO] Episode 9140\tAvg Length: 176.90\tAvg Reward: -2.811\n",
      "2025-01-27 02:07:09 [INFO] Episode 9160\tAvg Length: 148.30\tAvg Reward: -0.441\n",
      "2025-01-27 02:07:39 [INFO] Episode 9180\tAvg Length: 122.00\tAvg Reward: -3.146\n",
      "2025-01-27 02:08:09 [INFO] Episode 9200\tAvg Length: 121.00\tAvg Reward: -0.788\n",
      "2025-01-27 02:08:39 [INFO] Episode 9220\tAvg Length: 86.50\tAvg Reward: 2.658\n",
      "2025-01-27 02:09:09 [INFO] Episode 9240\tAvg Length: 120.90\tAvg Reward: -4.344\n",
      "2025-01-27 02:09:39 [INFO] Episode 9260\tAvg Length: 126.25\tAvg Reward: -0.217\n",
      "2025-01-27 02:10:09 [INFO] Episode 9280\tAvg Length: 140.45\tAvg Reward: -1.139\n",
      "2025-01-27 02:10:40 [INFO] Episode 9300\tAvg Length: 115.25\tAvg Reward: 0.321\n",
      "2025-01-27 02:11:10 [INFO] Episode 9320\tAvg Length: 142.90\tAvg Reward: -1.209\n",
      "2025-01-27 02:11:40 [INFO] Episode 9340\tAvg Length: 159.95\tAvg Reward: -3.551\n",
      "2025-01-27 02:12:10 [INFO] Episode 9360\tAvg Length: 112.70\tAvg Reward: 0.518\n",
      "2025-01-27 02:12:41 [INFO] Episode 9380\tAvg Length: 158.90\tAvg Reward: -0.936\n",
      "2025-01-27 02:13:11 [INFO] Episode 9400\tAvg Length: 154.30\tAvg Reward: 1.076\n",
      "2025-01-27 02:13:42 [INFO] Episode 9420\tAvg Length: 130.95\tAvg Reward: 0.160\n",
      "2025-01-27 02:14:12 [INFO] Episode 9440\tAvg Length: 137.05\tAvg Reward: 0.339\n",
      "2025-01-27 02:14:42 [INFO] Episode 9460\tAvg Length: 133.90\tAvg Reward: -2.044\n",
      "2025-01-27 02:15:12 [INFO] Episode 9480\tAvg Length: 111.20\tAvg Reward: 0.596\n",
      "2025-01-27 02:15:42 [INFO] Episode 9500\tAvg Length: 126.25\tAvg Reward: -5.223\n",
      "2025-01-27 02:16:12 [INFO] Episode 9520\tAvg Length: 126.00\tAvg Reward: 1.787\n",
      "2025-01-27 02:16:42 [INFO] Episode 9540\tAvg Length: 98.80\tAvg Reward: 0.248\n",
      "2025-01-27 02:17:13 [INFO] Episode 9560\tAvg Length: 120.40\tAvg Reward: -1.235\n",
      "2025-01-27 02:17:43 [INFO] Episode 9580\tAvg Length: 108.40\tAvg Reward: 2.662\n",
      "2025-01-27 02:18:13 [INFO] Episode 9600\tAvg Length: 140.05\tAvg Reward: 0.428\n",
      "2025-01-27 02:18:43 [INFO] Episode 9620\tAvg Length: 146.60\tAvg Reward: -0.214\n",
      "2025-01-27 02:19:14 [INFO] Episode 9640\tAvg Length: 145.55\tAvg Reward: -3.783\n",
      "2025-01-27 02:19:44 [INFO] Episode 9660\tAvg Length: 123.95\tAvg Reward: -3.016\n",
      "2025-01-27 02:20:14 [INFO] Episode 9680\tAvg Length: 157.60\tAvg Reward: -2.924\n",
      "2025-01-27 02:20:44 [INFO] Episode 9700\tAvg Length: 140.85\tAvg Reward: -0.695\n",
      "2025-01-27 02:21:14 [INFO] Episode 9720\tAvg Length: 132.70\tAvg Reward: -1.836\n",
      "2025-01-27 02:21:45 [INFO] Episode 9740\tAvg Length: 171.60\tAvg Reward: -2.237\n",
      "2025-01-27 02:22:15 [INFO] Episode 9760\tAvg Length: 155.60\tAvg Reward: -2.639\n",
      "2025-01-27 02:22:45 [INFO] Episode 9780\tAvg Length: 126.60\tAvg Reward: 1.865\n",
      "2025-01-27 02:23:15 [INFO] Episode 9800\tAvg Length: 86.25\tAvg Reward: 1.843\n",
      "2025-01-27 02:23:45 [INFO] Episode 9820\tAvg Length: 190.20\tAvg Reward: -4.649\n",
      "2025-01-27 02:24:16 [INFO] Episode 9840\tAvg Length: 142.55\tAvg Reward: -2.007\n",
      "2025-01-27 02:24:46 [INFO] Episode 9860\tAvg Length: 82.25\tAvg Reward: 2.987\n",
      "2025-01-27 02:25:16 [INFO] Episode 9880\tAvg Length: 103.75\tAvg Reward: 3.564\n",
      "2025-01-27 02:25:46 [INFO] Episode 9900\tAvg Length: 163.50\tAvg Reward: -0.553\n",
      "2025-01-27 02:26:16 [INFO] Episode 9920\tAvg Length: 133.05\tAvg Reward: -1.769\n",
      "2025-01-27 02:26:46 [INFO] Episode 9940\tAvg Length: 148.75\tAvg Reward: -3.858\n",
      "2025-01-27 02:27:16 [INFO] Episode 9960\tAvg Length: 160.00\tAvg Reward: -1.053\n",
      "2025-01-27 02:27:47 [INFO] Episode 9980\tAvg Length: 129.95\tAvg Reward: 1.469\n",
      "2025-01-27 02:28:17 [INFO] Saved checkpoint at episode 10000 -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/saved_models/DDPG_HockeyEnv_noise0.1_alr0.0001_clr0.0001_gamma0.95_checkpoint_ep10000.pth\n",
      "2025-01-27 02:28:17 [INFO] Saved training statistics to -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/stats/DDPG_HockeyEnv_noise0.1_alr0.0001_clr0.0001_gamma0.95_stats.pkl\n",
      "2025-01-27 02:28:17 [INFO] Episode 10000\tAvg Length: 145.65\tAvg Reward: -2.261\n",
      "2025-01-27 02:28:17 [INFO] Saved training statistics to -> rl_experiments/experiments/HockeyEnv_DDPG_Test/results/training/stats/DDPG_HockeyEnv_noise0.1_alr0.0001_clr0.0001_gamma0.95_stats.pkl\n",
      "2025-01-27 02:28:18 [INFO] Final training metrics: {'average_reward': -4.380339266631928, 'average_length': 146.681, 'final_loss_critic': 0.042895276099443436, 'final_loss_actor': -0.2940368056297302}\n",
      "Training finished.\n",
      "Final metrics: {'average_reward': -4.380339266631928, 'average_length': 146.681, 'final_loss_critic': 0.042895276099443436, 'final_loss_actor': -0.2940368056297302}\n"
     ]
    }
   ],
   "source": [
    "## This is only experimental, not sure if the hypothesis is valid\n",
    "\n",
    "final_metrics = trainer.train()\n",
    "\n",
    "print(\"Training finished.\")\n",
    "print(\"Final metrics:\", final_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cumulative reward: 4.77432291487633\n",
      "Win rate: 0.63\n"
     ]
    }
   ],
   "source": [
    "num_test_episodes = 100\n",
    "cumulative_rewards = []\n",
    "win_count = 0\n",
    "for ep in range(num_test_episodes):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0.0\n",
    "    while not done:\n",
    "        # set simple deterministic policy\n",
    "        action = trainer.agent.act(obs, evaluate=True)  \n",
    "        obs, reward, done, trunc, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "        env.render(mode=\"rgb_array\")\n",
    "\n",
    "        if done or trunc:\n",
    "            break\n",
    "    cumulative_rewards.append(episode_reward)\n",
    "    if info[\"winner\"] == 1:\n",
    "        win_count += 1\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"Average cumulative reward:\", np.mean(cumulative_rewards))\n",
    "print(\"Win rate:\", win_count / num_test_episodes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info: {'winner': -1, 'reward_closeness_to_puck': -0.08897551981456746, 'reward_touch_puck': 0.0, 'reward_puck_direction': -0.002343505096435547}\n"
     ]
    }
   ],
   "source": [
    "obs, reward, done, trunc, info = env.step(action)\n",
    "print(\"Info:\", info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x2/dvcm78hn67g79fdt399dzt8c0000gn/T/ipykernel_20009/4163330427.py:6: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# plot the cumulative rewards\n",
    "plt.plot(cumulative_rewards)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Cumulative Reward\")\n",
    "plt.title(\"Cumulative Rewards over Episodes\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 reward: 8.936073446347653 Won\n",
      "Episode 1 reward: -3.680941078652339 Lost\n",
      "Episode 2 reward: 5.686836507339108 Won\n",
      "Episode 3 reward: 7.939335089901141 Won\n",
      "Episode 4 reward: -13.075863483696557 Lost\n",
      "Episode 5 reward: 7.480006595146768 Won\n",
      "Episode 6 reward: 5.309186441633347 Won\n",
      "Episode 7 reward: 8.29429584536864 Won\n",
      "Episode 8 reward: -13.295177250518417 Lost\n",
      "Episode 9 reward: 9.459147403107348 Won\n",
      "Episode 10 reward: -10.851535856125562 Lost\n",
      "Episode 11 reward: 5.935956324338293 Won\n",
      "Episode 12 reward: -12.028975591788955 Lost\n",
      "Episode 13 reward: 8.241157593857107 Won\n",
      "Episode 14 reward: -6.4154907420834535 Lost\n",
      "Episode 15 reward: 9.301639333712199 Won\n",
      "Episode 16 reward: 9.153512559753235 Won\n",
      "Episode 17 reward: -5.145979633781056 Lost\n",
      "Episode 18 reward: -4.552991209537665 Lost\n",
      "Episode 19 reward: -4.069215832383605 Lost\n",
      "Episode 20 reward: -11.359597980848859 Lost\n",
      "Episode 21 reward: -13.188471066550333 Lost\n",
      "Episode 22 reward: -11.488474307694617 Lost\n",
      "Episode 23 reward: -2.224512168624859 Lost\n",
      "Episode 24 reward: -2.2670131611339013 Lost\n",
      "Episode 25 reward: 9.592786670257484 Won\n",
      "Episode 26 reward: -11.875465270423193 Lost\n",
      "Episode 27 reward: 9.232409279810604 Won\n",
      "Episode 28 reward: -4.299923109472145 Lost\n",
      "Episode 29 reward: -12.55209081294387 Lost\n",
      "Episode 30 reward: -13.141699880329622 Lost\n",
      "Episode 31 reward: -13.590265319348232 Lost\n",
      "Episode 32 reward: -3.546710491533207 Lost\n",
      "Episode 33 reward: -0.6216962597352104 Lost\n",
      "Episode 34 reward: -10.920593158558788 Lost\n",
      "Episode 35 reward: 9.34631287117386 Won\n",
      "Episode 36 reward: -10.772135539351458 Lost\n",
      "Episode 37 reward: -3.054323402178983 Lost\n",
      "Episode 38 reward: -3.650640736831828 Lost\n",
      "Episode 39 reward: 9.69055581242985 Won\n",
      "Episode 40 reward: -2.722426156736144 Lost\n",
      "Episode 41 reward: -12.900352878666446 Lost\n",
      "Episode 42 reward: -2.1319675761200223 Lost\n",
      "Episode 43 reward: -3.164355122611557 Lost\n",
      "Episode 44 reward: 7.007295071957351 Won\n",
      "Episode 45 reward: 8.63321671681367 Won\n",
      "Episode 46 reward: 9.801267854538903 Won\n",
      "Episode 47 reward: -1.9924505213686898 Lost\n",
      "Episode 48 reward: -4.884473917626987 Lost\n",
      "Episode 49 reward: -2.074694840167009 Lost\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "num_test_episodes = 50\n",
    "\n",
    "for ep in range(num_test_episodes):\n",
    "    frames = []\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0.0\n",
    "    while not done:\n",
    "        # Act with no noise at test time\n",
    "        action = trainer.agent.act(obs, evaluate=True)\n",
    "        obs, reward, done, trunc, info = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "        # ---- Capture frame in rgb_array mode ----\n",
    "        frame_rgb = env.render(mode='rgb_array')  \n",
    "        frames.append(frame_rgb)\n",
    "\n",
    "        if done or trunc:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    print(f\"Episode {ep} reward: {episode_reward}\", end=\" \")\n",
    "    if info[\"winner\"] == 1:\n",
    "        print(\"Won\")\n",
    "    else:\n",
    "        print(\"Lost\")\n",
    "\n",
    "\n",
    "    gif_path = f\"gifs/ddpg_laserhockey_episode{ep}.gif\"\n",
    "    imageio.mimsave(gif_path, frames, fps=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epsilon_start = 1.0  \n",
    "epsilon_end = 0.01  \n",
    "epsilon_decay = 0.999\n",
    "max_episodes=10000\n",
    "max_steps=500\n",
    "episodes = np.arange(max_episodes)\n",
    "epsilon_values = epsilon_start * (epsilon_decay ** episodes)\n",
    "epsilon_values = np.maximum(epsilon_values, epsilon_end)\n",
    "\n",
    "plt.plot(episodes, epsilon_values)\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Epsilon')\n",
    "plt.title('Epsilon Decay Over Episodes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved GIF to ddpg_laserhockey_episode.gif\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4) Save frames as GIF\n",
    "gif_path = \"ddpg_laserhockey_episode.gif\"\n",
    "imageio.mimsave(gif_path, frames, fps=15)  # set fps as desired\n",
    "print(f\"Saved GIF to {gif_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Training on Specialized curriculum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_curriculum():\n",
    "    # First we train Shooting\n",
    "    trainer = DDPGTrainer(env_name=\"HockeyEnv\",\n",
    "                          training_config=training_config,\n",
    "                          model_config=model_config,\n",
    "                          experiment_path=experiment_path,\n",
    "                          env_mode=\"train_shooting\",\n",
    "                          weak_opponent=True)\n",
    "    trainer.train()  # => here we store final checkpoint\n",
    "    \n",
    "    # Then we train Defense\n",
    "    trainer.env.close()\n",
    "    trainer.env_mode = \"train_defense\"\n",
    "    trainer.train()\n",
    "    \n",
    "    # Weak Opponent\n",
    "    trainer.env.close()\n",
    "    trainer.env_mode = \"normal\"\n",
    "    trainer.weak_opponent = True\n",
    "    trainer.train()\n",
    "    \n",
    "    # Normal Mode Strong Opponent\n",
    "    trainer.env.close()\n",
    "    trainer.weak_opponent = False\n",
    "    trainer.train()\n",
    "\n",
    "    # Maybe we can try self play with agent versions trained on the strong opponent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Self Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-29 12:44:54 [INFO] Logger initialized. Writing logs to rl_experiments/experiments/HockeyEnv_DDPG_SelfPlay/results/training/logs/DDPG_HockeyEnv_noise0.1_alr0.0001_clr0.0001_gamma0.95.log\n",
      "2025-01-29 12:44:54 [INFO] Initialized random seeds to 42.\n",
      "2025-01-29 12:44:54 [INFO] Logger initialized. Writing logs to rl_experiments/experiments/HockeyEnv_DDPG_SelfPlay/results/training/logs/DDPG_HockeyEnv_noise0.1_alr0.0001_clr0.0001_gamma0.95.log\n",
      "2025-01-29 12:44:54 [INFO] Initialized random seeds to 42.\n"
     ]
    }
   ],
   "source": [
    "training_config = {\n",
    "    \"env_mode\": Mode.NORMAL,\n",
    "    \"max_episodes\": 10000,     \n",
    "    \"max_timesteps\": 250,    \n",
    "    \"log_interval\": 20,\n",
    "    \"save_interval\": 1000,\n",
    "    \"render\": False,         \n",
    "    \"train_iter\": 32,       \n",
    "    \"seed\": 42      \n",
    "}\n",
    "\n",
    "model_config = {\n",
    "    \"noise_scale\": 0.1,                  \n",
    "    \"discount\": 0.95,      \n",
    "    \"buffer_size\": int(1e6),     \n",
    "    \"batch_size\": 128,            \n",
    "    \"learning_rate_actor\": 1e-4, \n",
    "    \"learning_rate_critic\": 1e-4,\n",
    "    \"hidden_sizes_actor\": [256, 256],\n",
    "    \"hidden_sizes_critic\": [256, 256],\n",
    "    \"update_target_every\": 100,\n",
    "    \"use_target_net\": True\n",
    "}\n",
    "\n",
    "experiment_path = \"rl_experiments/experiments/HockeyEnv_DDPG_SelfPlay\"\n",
    "\n",
    "# Initialize trainer\n",
    "agent1_trainer = DDPGTrainer(\n",
    "    env_name=\"HockeyEnv\",\n",
    "    training_config=training_config,\n",
    "    model_config=model_config,\n",
    "    experiment_path=experiment_path,\n",
    "    wandb_run=None \n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "agent2_trainer = DDPGTrainer(\n",
    "    env_name=\"HockeyEnv\",\n",
    "    training_config=training_config,\n",
    "    model_config=model_config,\n",
    "    experiment_path=experiment_path,\n",
    "    wandb_run=None \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x2/dvcm78hn67g79fdt399dzt8c0000gn/T/ipykernel_4447/3004529923.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path_agent1 = \"DDPG_HockeyEnv_noise0.1_alr0.0001_clr0.0001_gamma0.95_checkpoint_ep18000.pth\"\n",
    "checkpoint_path_agent2 = \"DDPG_HockeyEnv_noise0.1_alr0.0001_clr0.0001_gamma0.95_checkpoint_ep10000.pth\"\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "\n",
    "# restore the agent's networks\n",
    "agent1_trainer.agent.restore_state(checkpoint)\n",
    "agent2_trainer.agent.restore_state(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 finished. Reward: 9.792489565519515, Winner = 1\n",
      "Episode 1 finished. Reward: 9.448778922760745, Winner = 1\n",
      "Episode 2 finished. Reward: -15.23887846731606, Winner = -1\n",
      "Episode 3 finished. Reward: -11.383468760502128, Winner = -1\n",
      "Episode 4 finished. Reward: 5.865885343535432, Winner = 1\n",
      "Episode 5 finished. Reward: -4.022863715874989, Winner = 0\n",
      "Episode 6 finished. Reward: -6.695450138325012, Winner = 0\n",
      "Episode 7 finished. Reward: 8.983336789546476, Winner = 1\n",
      "Episode 8 finished. Reward: -12.62606995126547, Winner = -1\n",
      "Episode 9 finished. Reward: -5.386122795387592, Winner = 0\n",
      "Episode 10 finished. Reward: 8.217136980630261, Winner = 1\n",
      "Episode 11 finished. Reward: 7.911794398491437, Winner = 1\n",
      "Episode 12 finished. Reward: 6.44294508694367, Winner = 1\n",
      "Episode 13 finished. Reward: -14.316934382731905, Winner = -1\n",
      "Episode 14 finished. Reward: -13.078864247407871, Winner = -1\n",
      "Episode 15 finished. Reward: -14.929160359180122, Winner = -1\n",
      "Episode 16 finished. Reward: -10.512015200548431, Winner = -1\n",
      "Episode 17 finished. Reward: -12.852582468381097, Winner = -1\n",
      "Episode 18 finished. Reward: 6.785000229814264, Winner = 1\n",
      "Episode 19 finished. Reward: 8.718509393819488, Winner = 1\n",
      "Episode 20 finished. Reward: -13.32755103385028, Winner = -1\n",
      "Episode 21 finished. Reward: 8.736591020977826, Winner = 1\n",
      "Episode 22 finished. Reward: 5.029057758025201, Winner = 1\n",
      "Episode 23 finished. Reward: 9.537892923652805, Winner = 1\n",
      "Episode 24 finished. Reward: 9.971550905297955, Winner = 1\n",
      "Episode 25 finished. Reward: 8.073233502902397, Winner = 1\n",
      "Episode 26 finished. Reward: 9.570391158194015, Winner = 1\n",
      "Episode 27 finished. Reward: 9.657267579188112, Winner = 1\n",
      "Episode 28 finished. Reward: 9.813273386300073, Winner = 1\n",
      "Episode 29 finished. Reward: -17.060806334459254, Winner = -1\n",
      "Episode 30 finished. Reward: 9.968570654175467, Winner = 1\n",
      "Episode 31 finished. Reward: -6.209577127160718, Winner = 0\n",
      "Episode 32 finished. Reward: -13.815497567059694, Winner = -1\n",
      "Episode 33 finished. Reward: -11.465898452351492, Winner = -1\n",
      "Episode 34 finished. Reward: 8.355533267456252, Winner = 1\n",
      "Episode 35 finished. Reward: -13.982807945052922, Winner = -1\n",
      "Episode 36 finished. Reward: 9.199597330124783, Winner = 1\n",
      "Episode 37 finished. Reward: 9.375717041236225, Winner = 1\n",
      "Episode 38 finished. Reward: -14.678714685788364, Winner = -1\n",
      "Episode 39 finished. Reward: 7.441611824651596, Winner = 1\n",
      "Episode 40 finished. Reward: -10.865233320445334, Winner = -1\n",
      "Episode 41 finished. Reward: -13.245216302139243, Winner = -1\n",
      "Episode 42 finished. Reward: -10.911100467710403, Winner = -1\n",
      "Episode 43 finished. Reward: -15.639818970566782, Winner = -1\n",
      "Episode 44 finished. Reward: -10.979093886666915, Winner = -1\n",
      "Episode 45 finished. Reward: -7.140971471540313, Winner = 0\n",
      "Episode 46 finished. Reward: 8.085761076842724, Winner = 1\n",
      "Episode 47 finished. Reward: -12.41185838168919, Winner = -1\n",
      "Episode 48 finished. Reward: -10.842623079435642, Winner = -1\n",
      "Episode 49 finished. Reward: 8.99642118915711, Winner = 1\n"
     ]
    }
   ],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "num_test_episodes = 50\n",
    "# 1) Initialize environment\n",
    "env = HockeyEnv(keep_mode=True, mode=\"NORMAL\")  # or TRAIN_SHOOTING, etc.\n",
    "\n",
    "# 2) Load your two trained agents (DDPG #1 and DDPG #2)\n",
    "agent1 = agent1_trainer.agent\n",
    "agent2 = agent2_trainer.agent\n",
    "\n",
    "# 3) Run self-play episodes\n",
    "for episode in range(num_test_episodes):\n",
    "    frames = []\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0.0\n",
    "    while not done:\n",
    "        # (a) Observations for each agent\n",
    "        obs_agent1 = obs  # The default obs is always from player1's perspective\n",
    "        obs_agent2 = env.obs_agent_two()  # The environment provides a \"mirrored\" obs for player2\n",
    "        \n",
    "        # (b) Each agent selects an action\n",
    "        act1 = agent1.act(obs_agent1, evaluate=True)  # shape = (4,) if keep_mode=True\n",
    "        act2 = agent2.act(obs_agent2, evaluate=True)  # shape = (4,)\n",
    "\n",
    "        # (c) Concatenate into one 8D action: first 4 for player1, second 4 for player2\n",
    "        combined_act = np.hstack([act1, act2])  # shape = (8,)\n",
    "\n",
    "        # (d) Step the environment\n",
    "        next_obs, reward, done, trunc, info = env.step(combined_act)\n",
    "        episode_reward += reward\n",
    "\n",
    "        # (e) For multi-agent training, you might store transitions for each agent\n",
    "        #     But for a simple evaluation, you just continue\n",
    "        obs = next_obs\n",
    "\n",
    "        # ---- Capture frame in rgb_array mode ----\n",
    "        frame_rgb = env.render(mode='rgb_array')  \n",
    "        frames.append(frame_rgb)\n",
    "\n",
    "        if done or trunc:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    print(f\"Episode {episode} finished. Reward: {episode_reward}, Winner = {info['winner']}\")\n",
    "    gif_path = f\"rl_experiments/experiments/HockeyEnv_DDPG_SelfPlay/results/evaluation/gifs/SelfPlay_episode_{episode}.gif\"\n",
    "    imageio.mimsave(gif_path, frames, fps=15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Self Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "HockeyEnv_BasicOpponent.__init__() got an unexpected keyword argument 'keep_mode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m experiment_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrl_experiments/experiments/HockeyEnv_DDPG_SelfPlay\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Initialize trainer\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m trainer \u001b[38;5;241m=\u001b[39m DDPGTrainer(\n\u001b[1;32m     29\u001b[0m     env_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHockeyEnv\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     30\u001b[0m     env_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself_play\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     31\u001b[0m     training_config\u001b[38;5;241m=\u001b[39mtraining_config,\n\u001b[1;32m     32\u001b[0m     model_config\u001b[38;5;241m=\u001b[39mmodel_config,\n\u001b[1;32m     33\u001b[0m     experiment_path\u001b[38;5;241m=\u001b[39mexperiment_path,\n\u001b[1;32m     34\u001b[0m     wandb_run\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \n\u001b[1;32m     35\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Tübingen/WiSe24/ReinforcementLearning/Project/optimal-puck/models/ddpg/DDPGTrainer.py:33\u001b[0m, in \u001b[0;36mDDPGTrainer.__init__\u001b[0;34m(self, env_name, training_config, model_config, experiment_path, env_mode, wandb_run)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m env_mode \u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mself_play\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;241m=\u001b[39m HockeyEnv(keep_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNORMAL\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopp_env \u001b[38;5;241m=\u001b[39m HockeyEnv_BasicOpponent(keep_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNORMAL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopponent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_opponent()\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: HockeyEnv_BasicOpponent.__init__() got an unexpected keyword argument 'keep_mode'"
     ]
    }
   ],
   "source": [
    "training_config = {\n",
    "    \"env_mode\": Mode.NORMAL,\n",
    "    \"max_episodes\": 10000,     \n",
    "    \"max_timesteps\": 250,    \n",
    "    \"log_interval\": 20,\n",
    "    \"save_interval\": 1000,\n",
    "    \"render\": False,         \n",
    "    \"train_iter\": 32,       \n",
    "    \"seed\": 42      \n",
    "}\n",
    "\n",
    "model_config = {\n",
    "    \"noise_scale\": 0.1,                  \n",
    "    \"discount\": 0.95,      \n",
    "    \"buffer_size\": int(1e6),     \n",
    "    \"batch_size\": 128,            \n",
    "    \"learning_rate_actor\": 1e-4, \n",
    "    \"learning_rate_critic\": 1e-4,\n",
    "    \"hidden_sizes_actor\": [256, 256],\n",
    "    \"hidden_sizes_critic\": [256, 256],\n",
    "    \"update_target_every\": 100,\n",
    "    \"use_target_net\": True\n",
    "}\n",
    "\n",
    "experiment_path = \"rl_experiments/experiments/HockeyEnv_DDPG_SelfPlay\"\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = DDPGTrainer(\n",
    "    env_name=\"HockeyEnv\",\n",
    "    env_mode=\"self_play\",\n",
    "    training_config=training_config,\n",
    "    model_config=model_config,\n",
    "    experiment_path=experiment_path,\n",
    "    wandb_run=None \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_metrics = trainer.train()\n",
    "\n",
    "print(\"Training finished.\")\n",
    "print(\"Final metrics:\", final_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x2/dvcm78hn67g79fdt399dzt8c0000gn/T/ipykernel_4447/3667419123.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"NewAugmentedReward_DDPG_Agent.pth\"\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
    "\n",
    "# restore the agent's networks\n",
    "trainer.agent.restore_state(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "\n",
    "num_test_episodes = 50\n",
    "# 1) Initialize environment\n",
    "env = HockeyEnv(keep_mode=True, mode=\"NORMAL\")  # or TRAIN_SHOOTING, etc.\n",
    "\n",
    "# 2) Load your two trained agents (DDPG #1 and DDPG #2)\n",
    "agent1 = agent1_trainer.agent\n",
    "agent2 = agent2_trainer.agent\n",
    "\n",
    "# 3) Run self-play episodes\n",
    "for episode in range(num_test_episodes):\n",
    "    frames = []\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0.0\n",
    "    while not done:\n",
    "        # (a) Observations for each agent\n",
    "        obs_agent1 = obs  # The default obs is always from player1's perspective\n",
    "        obs_agent2 = env.obs_agent_two()  # The environment provides a \"mirrored\" obs for player2\n",
    "        \n",
    "        # (b) Each agent selects an action\n",
    "        act1 = agent1.act(obs_agent1, evaluate=True)  # shape = (4,) if keep_mode=True\n",
    "        act2 = agent2.act(obs_agent2, evaluate=True)  # shape = (4,)\n",
    "\n",
    "        # (c) Concatenate into one 8D action: first 4 for player1, second 4 for player2\n",
    "        combined_act = np.hstack([act1, act2])  # shape = (8,)\n",
    "\n",
    "        # (d) Step the environment\n",
    "        next_obs, reward, done, trunc, info = env.step(combined_act)\n",
    "        episode_reward += reward\n",
    "\n",
    "        # (e) For multi-agent training, you might store transitions for each agent\n",
    "        #     But for a simple evaluation, you just continue\n",
    "        obs = next_obs\n",
    "\n",
    "        # ---- Capture frame in rgb_array mode ----\n",
    "        frame_rgb = env.render(mode='rgb_array')  \n",
    "        frames.append(frame_rgb)\n",
    "\n",
    "        if done or trunc:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "    print(f\"Episode {episode} finished. Reward: {episode_reward}, Winner = {info['winner']}\")\n",
    "    gif_path = f\"AugmentedRewardGifs/AugmentedReward_episode_{episode}.gif\"\n",
    "    imageio.mimsave(gif_path, frames, fps=15)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
